<!DOCTYPE html>
<html lang="en">
<head>
  <!-- <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'> -->
   <link href='//fonts.proxy.ustclug.org/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/style.min.css">
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/pygments.min.css">
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/font-awesome.min.css">
  <link href="http://littlezz.github.io/static/custom.css" rel="stylesheet">
  <link href="http://littlezz.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="littlezz's Blog Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="littlezz" />
<meta name="description" content="Intro I'm working on a project named ESL-Model which I want to use python to implent algorithm from The Elements of Statistical Learning. I start writing RRLDA the day before yesterday, and I sccuessfully finish until today. Notice that I am using Python3.5, which allow me use @ instead of np.dot The difference between Numpy and R Firstly, Numpy is slight different with R in eigen decomposition(I spent lots of time to find out T_T). In R, the eigenvalues return from eigen is descending, which is all notes …" />
<meta name="keywords" content="machine learning, 机器学习, python, 小结, numpy">
<meta property="og:site_name" content="littlezz's Blog"/>
<meta property="og:title" content="How to Write Reduced Rank Linear Discriminant Analysis with Python"/>
<meta property="og:description" content="Intro I'm working on a project named ESL-Model which I want to use python to implent algorithm from The Elements of Statistical Learning. I start writing RRLDA the day before yesterday, and I sccuessfully finish until today. Notice that I am using Python3.5, which allow me use @ instead of np.dot The difference between Numpy and R Firstly, Numpy is slight different with R in eigen decomposition(I spent lots of time to find out T_T). In R, the eigenvalues return from eigen is descending, which is all notes …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="http://littlezz.github.io/how-to-write-reduced-rank-linear-discriminant-analysis-with-python.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2016-03-26 00:00:00+08:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="http://littlezz.github.io/author/littlezz.html">
<meta property="article:section" content="Machine-Learning"/>
<meta property="article:tag" content="machine learning"/>
<meta property="article:tag" content="机器学习"/>
<meta property="article:tag" content="python"/>
<meta property="article:tag" content="小结"/>
<meta property="article:tag" content="numpy"/>
<meta property="og:image" content="/images/logo.jpg">  <title>littlezz's Blog &ndash; How to Write Reduced Rank Linear Discriminant Analysis with Python</title>
</head>
<body>
  <aside>
    <div>
      <a href="http://littlezz.github.io">
        <img src="/images/logo.jpg" alt="littlezz" title="littlezz">
      </a>
      <h1><a href="http://littlezz.github.io">littlezz</a></h1>
      <p>Everything that has a beginning has an end.</p>
      <nav>
        <ul class="list">
        </ul>
      </nav>
      <ul class="social">
        <li><a class="sc-github" href="https://github.com/littlezz/" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>
  </aside>
  <main>
    <nav>
      <a href="http://littlezz.github.io">Home</a>
      <a href="/categories">Category</a>
      <a href="/tags">Tags</a>
      <a href="http://littlezz.github.io/feeds/all.atom.xml">Atom</a>
    </nav>

<article>
  <header>
    <h1 id="how-to-write-reduced-rank-linear-discriminant-analysis-with-python">How to Write Reduced Rank Linear Discriminant Analysis with Python</h1>
    <p>Posted on Sat 26 March 2016 in <a href="http://littlezz.github.io/category/machine-learning.html">Machine-Learning</a></p>
  </header>
  <div>
    <h2>Intro</h2>
<p>I'm working on a project named <a href="https://github.com/littlezz/ESL-Model">ESL-Model</a> which I want to use python to implent algorithm from The Elements of Statistical Learning. I start writing RRLDA the day before yesterday,  and I sccuessfully finish until today.<br>
</p>
<p>Notice that I am using Python3.5, which allow me use <code>@</code> instead of <code>np.dot</code><br>
</p>
<h2>The difference between Numpy and R</h2>
<p>Firstly, Numpy is slight different with R in eigen decomposition(I spent lots of time to find out T_T).<br>
</p>
<p>In R, the eigenvalues return from <code>eigen</code> is descending, which is all notes assume.<br>
But Numpy <code>np.linalg.eigh</code> return eigenvalues in ascending order.<br>
</p>
<p>It is doesn't matter when you work on LDA, because it use both $V$ and $D$, but it influence the result in Reduced rank LDA which you use column of $V$ alone.<br>
</p>
<p>To slove this problem, you may use <code>np.fliplr</code>.<br>
</p>
<div class="highlight"><pre><span></span><span class="n">W</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.9967</span><span class="p">,</span>  <span class="mf">0.002</span> <span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.002</span> <span class="p">,</span>  <span class="mf">1.0263</span><span class="p">]])</span>

<span class="n">Dw</span><span class="p">,</span> <span class="n">Uw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
<span class="n">Dw</span> <span class="o">=</span> <span class="n">Dw</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Uw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fliplr</span><span class="p">(</span><span class="n">Uw</span><span class="p">)</span>
</pre></div>


<p>You can get </p>
<div class="highlight"><pre><span></span>print(Dw)
[ 1.02643452  0.99656548]

print(Dw)
[[ 0.06711024 -0.99774557]
 [ 0.99774557  0.06711024]]
</pre></div>


<h2>How to Write Reduced Rank LDA</h2>
<p>Finally, we arrive at the algorithm.<br>
I mainly reference http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/lda2.pdf. <br>
Which is a very very nice ppt, thanks it very much!<br>
</p>
<ul>
<li>
<p>Find the centroids for all the classes and class prior probabilities, which is the same in LDA ($\mu_k$ and $\pi_k$).<br>
</p>
</li>
<li>
<p>Find between-class covariance matrix B using the centroid vectors and class prior probabilities<br>
    suppose $\mu_k$ is column vector</p>
<p>$\mu = \Sigma_1^K \mu_k$<br>
$B =  \Sigma_1^K \pi_k(\mu - \mu_k)(\mu - \mu_k)^T$</p>
<p><strong>Notice that the code dosen't take same shape with formula</strong><br>
</p>
<p>```python
W = self.Sigma_hat</p>
<h1>prior probabilities (K,1)</h1>
<p>Pi = self.Pi</p>
<h1>class centroids (K, p)</h1>
<p>Mu = self.Mu
p = self.p</p>
<h1>the number of class</h1>
<p>K = self.K</p>
<h1>the dimension you want</h1>
<p>L = self.L</p>
<h1>Mu is (K,p) matrix, Pi is (K,1)</h1>
<p>mu = np.sum(Pi * Mu, axis=0)
B = np.zeros((p, p))</p>
<p>for k in range(K):
    # vector @ vector equal scalar, use vector[:, None] to transform to matrix
    # vec[:, None] equal to vec.reshape((1, vec.shape[0]))
    B = B + Pi[k]*((Mu[k] - mu)[:, None] @ ((Mu[k] - mu)[None, :]))
```</p>
</li>
<li>
<p>Find within-class covariance matrix W, the same with $\hat \Sigma$ in LDA.<br>
</p>
</li>
<li>
<p>eigen decomposition W</p>
<p>$W = V_WD_WV_W^T$<br>
Define $W^{1/2} = D_W^{1/2}V_W^T$</p>
<p>So $W^{-\frac{1}{2}} = (W^{1/2})^{-1}$<br>
</p>
<p>```python</p>
<h1>Be careful, the <code>eigh</code> method get the eigenvalues in ascending , which is opposite to R.</h1>
<p>Dw, Uw = LA.eigh(W)</p>
<h1>reverse the Dw_ and Uw</h1>
<p>Dw = Dw[::-1]
Uw = np.fliplr(Uw)</p>
<p>W_half = np.linalg.pinv(np.diagflat(Dw**0.5) @ Uw.T)
```</p>
</li>
<li>
<p>compute $B^<em>$<br>
    $B^</em> = (W^{-\frac{1}{2}})^TBW^{-\frac{1}{2}}$<br>
</p>
<p>```python<br>
B_star = W_half.T @ B @ W_half</p>
<p>```</p>
</li>
<li>
<p>eigen decompostion $B^<em>$, get $a_l$<br>
    $B^</em> = VDV^T$<br>
    $a_l = W^{-\frac{1}{2}}V_l$</p>
<p>```python<br>
D_, V = LA.eigh(B_star)</p>
<h1>reverse V</h1>
<p>V = np.fliplr(V)</p>
<p>A = np.zeros((L, p))
for l in range(L):
    A[l, :] = (W_half) @ V[:, l]
```</p>
</li>
<li>
<p>transform X and $\mu_k$ and get predict value
    $\hat x = Ax$<br>
    $\hat \mu_k = A\mu_k$</p>
<p>find $argmin_k {|\hat x- \hat\mu|_2^2 - log(\pi_k)}$</p>
<p>```python
X_star = X @ A.T</p>
<p>for k in range(self.K):
    # mu_s_star shape is (p,)
    mu_k_star = A @ self.Mu[k]</p>
<div class="highlight"><pre><span></span># Ref: http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html
# Ref: http://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy
Y[:, k] = LA.norm(X_star - mu_k_star, axis=1) * 0.5 - log(self.Pi[k])
</pre></div>


<h1>Python index start from 0, transform to start with 1</h1>
<p>y_hat = Y.argmin(axis=1).reshape((-1, 1)) + 1
```</p>
</li>
</ul>
<h2>Put it Together</h2>
<p>You can find complete code in here
https://github.com/littlezz/ESL-Model/blob/master/esl_model/ch4/model.py</p>
<h2>Reference</h2>
<ul>
<li>The Elements of Statistical Learning 2nd Edition section 4.3.2 - 4.3.3<br>
</li>
<li>http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/lda2.pdf<br>
</li>
<li>https://onlinecourses.science.psu.edu/stat857/node/83</li>
<li>http://www.stat.cmu.edu/~ryantibs/datamining/lectures/21-clas2-marked.pdf</li>
</ul>
<p>2016-03-26 15:43:45</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="http://littlezz.github.io/tag/machine-learning.html">machine learning</a>
      <a href="http://littlezz.github.io/tag/ji-qi-xue-xi.html">机器学习</a>
      <a href="http://littlezz.github.io/tag/python.html">python</a>
      <a href="http://littlezz.github.io/tag/xiao-jie.html">小结</a>
      <a href="http://littlezz.github.io/tag/numpy.html">numpy</a>
    </p>
  </div>
</article>

    <footer>
        <p>&copy; littlezz 2017</p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "How to Write Reduced Rank Linear Discriminant Analysis with Python",
  "headline": "How to Write Reduced Rank Linear Discriminant Analysis with Python",
  "datePublished": "2016-03-26 00:00:00+08:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "littlezz",
    "url": "http://littlezz.github.io/author/littlezz.html"
  },
  "image": "/images/logo.jpg",
  "url": "http://littlezz.github.io/how-to-write-reduced-rank-linear-discriminant-analysis-with-python.html",
  "description": "Intro I'm working on a project named ESL-Model which I want to use python to implent algorithm from The Elements of Statistical Learning. I start writing RRLDA the day before yesterday, and I sccuessfully finish until today. Notice that I am using Python3.5, which allow me use @ instead of np.dot The difference between Numpy and R Firstly, Numpy is slight different with R in eigen decomposition(I spent lots of time to find out T_T). In R, the eigenvalues return from eigen is descending, which is all notes …"
}
</script></body>
</html>