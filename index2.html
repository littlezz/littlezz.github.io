<!DOCTYPE html>
<html lang="en">
<head>
  <!-- <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'> -->
   <link href='//fonts.proxy.ustclug.org/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/style.min.css">
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/pygments.min.css">
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/font-awesome.min.css">
  <link href="http://littlezz.github.io/static/custom.css" rel="stylesheet">
  <link href="http://littlezz.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="littlezz's Blog Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
  <meta name="author" content="littlezz" />
  <meta name="description" content="littlezz's Thoughts and Writings About Python And Machine Learning and diary" />
<meta property="og:site_name" content="littlezz's Blog"/>
<meta property="og:type" content="blog"/>
<meta property="og:title" content="littlezz's Blog"/>
<meta property="og:description" content="littlezz's Thoughts and Writings About Python And Machine Learning and diary"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="http://littlezz.github.io"/>
<meta property="og:image" content="/images/logo.jpg">
  <title>littlezz's Blog</title>
</head>
<body>
  <aside>
    <div>
      <a href="http://littlezz.github.io">
        <img src="/images/logo.jpg" alt="littlezz" title="littlezz">
      </a>
      <h1><a href="http://littlezz.github.io">littlezz</a></h1>
      <p>Everything that has a beginning has an end.</p>
      <nav>
        <ul class="list">
        </ul>
      </nav>
      <ul class="social">
        <li><a class="sc-github" href="https://github.com/littlezz/" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>
  </aside>
  <main>
    <nav>
      <a href="http://littlezz.github.io">Home</a>
      <a href="/categories">Category</a>
      <a href="/tags">Tags</a>
      <a href="http://littlezz.github.io/feeds/all.atom.xml">Atom</a>
    </nav>

<article>
  <header>
    <h2><a href="http://littlezz.github.io/guan-yu-wo-bu-zai-de-jie-dao-ping-fen-de-yi-dian-si-kao.html#guan-yu-wo-bu-zai-de-jie-dao-ping-fen-de-yi-dian-si-kao">关于《我不在的街道》评分的一点思考</a></h2>
    <p>
      Posted on Mon 04 April 2016 in <a href="http://littlezz.github.io/category/acg.html">ACG</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/ri-ji.html">日记</a>,      <a href="http://littlezz.github.io/tag/xia-che.html">瞎扯</a>,      <a href="http://littlezz.github.io/tag/xiao-jie.html">小结</a>    </p>
  </header>
  <div>
      <p>我给了9分（神作）<br>
</p>
<p>刚看完的时候， 对于那个操蛋的结尾， 我也是想打七分的， 但是觉得实在太低， 对不起那个惊艳的开头， 于是给八分。<br>
</p>
<p>过了1天， 我连续半话弃掉了3部番， 其中包括《fate stay night UBW》， 我在群里面说， 『不行， 刚看完扑街， 其他番根本没有办法看， 全部半话弃』<br>
直到此刻， 我才明白， 有多少番， 我甚至没有办法看完第一话， 更不用说考虑什么剧情了。<br>
</p>
<p>我开始察觉到自己， 以及大部分人的可笑。<br>
一副严肃认真， 公平正义的态度， 打上一个完全主观的分数， 留下一句似是而非的自己评分准则。<br>
俨然一副资深人士的样子。<br>
</p>
<p>然而来来回回， 不还是局限在剧情方面？<br>
一部动画， 构图， 分镜， 配音， 上色， 配乐等等， 这么多元素在里面， 最后大家却振振有词的依据剧情来打分， 还能说得头头是道。<br>
</p>
<p>我才明白当时的我也是如此， 也就是在那时， 我才会想起我起初看这个动画的时候的兴奋之情， 我大呼卧槽， 我呼啦一下打开房间的门， 我走到从来不看番的室友的房间， 我说， 卧槽， 我要通宵看完这部番了， 我已经有几年没有这种想要通宵看番的冲动了， 上一次这样还是高中， 看得是石头门。<br>
我在一集结束的时候， 高呼悠木碧， 棒啊啊啊啊。 我说， fuck them all！ 到后面， 我室友都跑过来我们两个熬夜看番了。<br>
</p>
<p>也就是那时， 我才会想起， 有多少回， 我被里面的一幕触动到， 我说， 就凭此刻， 足以封神。 <br>
</p>
<p>所以我改成了9分， 为了我当初的承诺， 为了第二话那句『原来我也有过这样的时间啊』， 为了在记录那时感想时触动的回忆让我流着泪写完观后感。<br>
</p>
<p>如果此刻我还保持着一副正人君子， 道貌岸然， 满口胡言的扯着剧情的『公平之人』， 那我就是一个双料的傻瓜。<br>
</p>
<p>干他娘的， 我就是要给9分， 有傻逼打低分， 为什么我就不能打高分？<br>
</p>
<p>说到底， 为什么作品的排名会被高低分这种东西牵制。 或者说如何排除傻逼对一个作品的影响。<br>
</p>
<p>投票者投票表决天才， 但是有一个前提是， 如何保证投票者本身不是平庸之流。<br>
</p>
<p>抛开知乎这种社交性质的网站不谈， 如果是这种个人评分性质，带有很重的个人主观口味性质的，但是最后却是统计平均数的网站来看。 应该首先匹配评分者中和当前用户匹配率高的用户， 重新分配权重。<br>
</p>
<p>咦， 这不就是协同过滤么。。。<br>
</p>
<p>说来， 要先解决计算用户之间的距离的问题， 然后重新分配权重， 然后重新得到分数， 卧槽， 简单啊。<br>
</p>
<p>我是不是可以去写一个啊。。。？<br>
</p>
<p>看情况， 先把之前的坑给填了先。<br>
</p>
<p>2016年04月04日19:37:51</p>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/travis-ci-with-numpy-and-pytest.html#travis-ci-with-numpy-and-pytest">Travis CI with Numpy and Pytest</a></h2>
    <p>
      Posted on Thu 31 March 2016 in <a href="http://littlezz.github.io/category/python.html">Python</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/python.html">python</a>,      <a href="http://littlezz.github.io/tag/numpy.html">numpy</a>,      <a href="http://littlezz.github.io/tag/xiao-jie.html">小结</a>    </p>
  </header>
  <div>
      <p>It takes long time to install scientific python package by <code>pip</code> in travis and sometime fail.<br>
</p>
<p>The solve is to use conda to install binary package. <br>
<a href="http://conda.pydata.org/docs/travis.html">http://conda.pydata.org/docs/travis.html</a><br>
</p>
<p>Note that some package not in conda, you can use <code>pip</code> install it, for example, <code>py.test</code><br>
</p>
<p>Here is my <a href="https://github.com/littlezz/ESL-Model">ESL-Model</a> .trais.yml, you can view on github <a href="https://github.com/littlezz/ESL-Model/blob/master/.travis.yml">https://github.com/littlezz/ESL-Model/blob/master/.travis.yml</a><br>
</p>
<div class="highlight"><pre><span></span>language: python
python:
  <span class="c1"># We don&#39;t actually use the Travis Python, but this keeps it organized.</span>
  - <span class="s2">&quot;3.5&quot;</span>

install:
  - sudo apt-get update
  <span class="c1"># We do this conditionally because it saves us some downloading if the</span>
  <span class="c1"># version is the same.</span>
  - <span class="k">if</span> <span class="o">[[</span> <span class="s2">&quot;</span><span class="nv">$TRAVIS_PYTHON_VERSION</span><span class="s2">&quot;</span> <span class="o">==</span> <span class="s2">&quot;2.7&quot;</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
      wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O miniconda.sh<span class="p">;</span>
    <span class="k">else</span>
      wget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh<span class="p">;</span>
    <span class="k">fi</span>
  - bash miniconda.sh -b -p <span class="nv">$HOME</span>/miniconda
  - <span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$HOME</span><span class="s2">/miniconda/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
  - <span class="nb">hash</span> -r
  - conda config --set always_yes yes --set changeps1 no
  - conda update -q conda
  <span class="c1"># Useful for debugging any issues with conda</span>
  - conda info -a

  - conda create -q -n test-environment <span class="nv">python</span><span class="o">=</span><span class="nv">$TRAVIS_PYTHON_VERSION</span> pandas numpy scipy scikit-learn
  - <span class="nb">source</span> activate test-environment
  - python setup.py install
  - pip install pytest

script:
  - py.test

notifications:
  email:
    on_success: never
    on_failure: always
</pre></div>


<p>Note that you should install pytest manually instead use pre-install one.</p>
<p>However, you can not use tox with conda direct, try ctox <a href="https://github.com/hayd/ctox">https://github.com/hayd/ctox</a><br>
</p>
<p>2016-03-31 16:19:38</p>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/how-to-write-reduced-rank-linear-discriminant-analysis-with-python.html#how-to-write-reduced-rank-linear-discriminant-analysis-with-python">How to Write Reduced Rank Linear Discriminant Analysis with Python</a></h2>
    <p>
      Posted on Sat 26 March 2016 in <a href="http://littlezz.github.io/category/machine-learning.html">Machine-Learning</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/machine-learning.html">machine learning</a>,      <a href="http://littlezz.github.io/tag/ji-qi-xue-xi.html">机器学习</a>,      <a href="http://littlezz.github.io/tag/python.html">python</a>,      <a href="http://littlezz.github.io/tag/xiao-jie.html">小结</a>,      <a href="http://littlezz.github.io/tag/numpy.html">numpy</a>    </p>
  </header>
  <div>
      <h2>Intro</h2>
<p>I'm working on a project named <a href="https://github.com/littlezz/ESL-Model">ESL-Model</a> which I want to use python to implent algorithm from The Elements of Statistical Learning. I start writing RRLDA the day before yesterday,  and I sccuessfully finish until today.<br>
</p>
<p>Notice that I am using Python3.5, which allow me use <code>@</code> instead of <code>np.dot</code><br>
</p>
<h2>The difference between Numpy and R</h2>
<p>Firstly, Numpy is slight different with R in eigen decomposition(I spent lots of time to find out T_T).<br>
</p>
<p>In R, the eigenvalues return from <code>eigen</code> is descending, which is all notes assume.<br>
But Numpy <code>np.linalg.eigh</code> return eigenvalues in ascending order.<br>
</p>
<p>It is doesn't matter when you work on LDA, because it use both $V$ and $D$, but it influence the result in Reduced rank LDA which you use column of $V$ alone.<br>
</p>
<p>To slove this problem, you may use <code>np.fliplr</code>.<br>
</p>
<div class="highlight"><pre><span></span><span class="n">W</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.9967</span><span class="p">,</span>  <span class="mf">0.002</span> <span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.002</span> <span class="p">,</span>  <span class="mf">1.0263</span><span class="p">]])</span>

<span class="n">Dw</span><span class="p">,</span> <span class="n">Uw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
<span class="n">Dw</span> <span class="o">=</span> <span class="n">Dw</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Uw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fliplr</span><span class="p">(</span><span class="n">Uw</span><span class="p">)</span>
</pre></div>


<p>You can get </p>
<div class="highlight"><pre><span></span>print(Dw)
[ 1.02643452  0.99656548]

print(Dw)
[[ 0.06711024 -0.99774557]
 [ 0.99774557  0.06711024]]
</pre></div>


<h2>How to Write Reduced Rank LDA</h2>
<p>Finally, we arrive at the algorithm.<br>
I mainly reference http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/lda2.pdf. <br>
Which is a very very nice ppt, thanks it very much!<br>
</p>
<ul>
<li>
<p>Find the centroids for all the classes and class prior probabilities, which is the same in LDA ($\mu_k$ and $\pi_k$).<br>
</p>
</li>
<li>
<p>Find between-class covariance matrix B using the centroid vectors and class prior probabilities<br>
    suppose $\mu_k$ is column vector</p>
<p>$\mu = \Sigma_1^K \mu_k$<br>
$B =  \Sigma_1^K \pi_k(\mu - \mu_k)(\mu - \mu_k)^T$</p>
<p><strong>Notice that the code dosen't take same shape with formula</strong><br>
</p>
<p>```python
W = self.Sigma_hat</p>
<h1>prior probabilities (K,1)</h1>
<p>Pi = self.Pi</p>
<h1>class centroids (K, p)</h1>
<p>Mu = self.Mu
p = self.p</p>
<h1>the number of class</h1>
<p>K = self.K</p>
<h1>the dimension you want</h1>
<p>L = self.L</p>
<h1>Mu is (K,p) matrix, Pi is (K,1)</h1>
<p>mu = np.sum(Pi * Mu, axis=0)
B = np.zeros((p, p))</p>
<p>for k in range(K):
    # vector @ vector equal scalar, use vector[:, None] to transform to matrix
    # vec[:, None] equal to vec.reshape((1, vec.shape[0]))
    B = B + Pi[k]*((Mu[k] - mu)[:, None] @ ((Mu[k] - mu)[None, :]))
```</p>
</li>
<li>
<p>Find within-class covariance matrix W, the same with $\hat \Sigma$ in LDA.<br>
</p>
</li>
<li>
<p>eigen decomposition W</p>
<p>$W = V_WD_WV_W^T$<br>
Define $W^{1/2} = D_W^{1/2}V_W^T$</p>
<p>So $W^{-\frac{1}{2}} = (W^{1/2})^{-1}$<br>
</p>
<p>```python</p>
<h1>Be careful, the <code>eigh</code> method get the eigenvalues in ascending , which is opposite to R.</h1>
<p>Dw, Uw = LA.eigh(W)</p>
<h1>reverse the Dw_ and Uw</h1>
<p>Dw = Dw[::-1]
Uw = np.fliplr(Uw)</p>
<p>W_half = np.linalg.pinv(np.diagflat(Dw**0.5) @ Uw.T)
```</p>
</li>
<li>
<p>compute $B^<em>$<br>
    $B^</em> = (W^{-\frac{1}{2}})^TBW^{-\frac{1}{2}}$<br>
</p>
<p>```python<br>
B_star = W_half.T @ B @ W_half</p>
<p>```</p>
</li>
<li>
<p>eigen decompostion $B^<em>$, get $a_l$<br>
    $B^</em> = VDV^T$<br>
    $a_l = W^{-\frac{1}{2}}V_l$</p>
<p>```python<br>
D_, V = LA.eigh(B_star)</p>
<h1>reverse V</h1>
<p>V = np.fliplr(V)</p>
<p>A = np.zeros((L, p))
for l in range(L):
    A[l, :] = (W_half) @ V[:, l]
```</p>
</li>
<li>
<p>transform X and $\mu_k$ and get predict value
    $\hat x = Ax$<br>
    $\hat \mu_k = A\mu_k$</p>
<p>find $argmin_k {|\hat x- \hat\mu|_2^2 - log(\pi_k)}$</p>
<p>```python
X_star = X @ A.T</p>
<p>for k in range(self.K):
    # mu_s_star shape is (p,)
    mu_k_star = A @ self.Mu[k]</p>
<div class="highlight"><pre><span></span># Ref: http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html
# Ref: http://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy
Y[:, k] = LA.norm(X_star - mu_k_star, axis=1) * 0.5 - log(self.Pi[k])
</pre></div>


<h1>Python index start from 0, transform to start with 1</h1>
<p>y_hat = Y.argmin(axis=1).reshape((-1, 1)) + 1
```</p>
</li>
</ul>
<h2>Put it Together</h2>
<p>You can find complete code in here
https://github.com/littlezz/ESL-Model/blob/master/esl_model/ch4/model.py</p>
<h2>Reference</h2>
<ul>
<li>The Elements of Statistical Learning 2nd Edition section 4.3.2 - 4.3.3<br>
</li>
<li>http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/lda2.pdf<br>
</li>
<li>https://onlinecourses.science.psu.edu/stat857/node/83</li>
<li>http://www.stat.cmu.edu/~ryantibs/datamining/lectures/21-clas2-marked.pdf</li>
</ul>
<p>2016-03-26 15:43:45</p>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/fuck-the-reduced-rank-lda.html#fuck-the-reduced-rank-lda">Fuck the Reduced-rank LDA</a></h2>
    <p>
      Posted on Fri 25 March 2016 in <a href="http://littlezz.github.io/category/machine-learning.html">Machine-Learning</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/machine-learning.html">machine learning</a>,      <a href="http://littlezz.github.io/tag/ji-qi-xue-xi.html">机器学习</a>,      <a href="http://littlezz.github.io/tag/ta-ma-de.html">他妈的</a>,      <a href="http://littlezz.github.io/tag/xue-xue-xue-xue-ge-pi.html">学学学，学个屁</a>    </p>
  </header>
  <div>
      <p>从昨天开始一直在弄RRLDA， 直到今天晚上， 无果。<br>
搜索各种资料， 各自有理， 到最后连计算 $W^{-\frac{1}{2}}$ 都能有不同的版本。<br>
</p>
<p>权当整理思路吧， 整理一下各种版本（可能我编程有问题， 反正所有版本我都没能算对：）<br>
</p>
<h2>版本一： ESL</h2>
<p>page 114 页的算法。<br>
我翻译一下：<br>
</p>
<ul>
<li>计算 类的重心 K x P 的矩阵M， 还有公共协方差矩阵W；</li>
<li>计算 $M^* = MW^{-\frac{1}{2}}$, 使用W的特征分解；</li>
<li>计算$B^<em>$, $M^</em>$的协方差矩阵（B是为了between-class）, 还有他的特征分解 $B^<em> = V^</em>D_BV^{<em>T}$. $V^</em>$的按顺序的列$v_l^*$定义了最佳子空间的坐标。<br>
</li>
</ul>
<p>卧槽， 我他妈看懂了。<br>
我往下看了一行。 
书上的意思是说， 你本来要算这些jb玩意的， 但是后来有一个叫做『捕鱼者（fisher）』的捕鱼达人， 给出了更吊的东西， 卧槽， 我他妈真的咸鱼啊， 这个人一波收割啊卧槽。<br>
</p>
<p>那我懂了， 我接着看书了。<br>
</p>
<p>FUCK！<br>
FUCK ME！</p>
<p>2016年03月25日00:46:12</p>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/numpy-shi-yong-xiao-jie.html#numpy-shi-yong-xiao-jie">Numpy 使用小结</a></h2>
    <p>
      Posted on Mon 21 March 2016 in <a href="http://littlezz.github.io/category/python.html">Python</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/numpy.html">numpy</a>,      <a href="http://littlezz.github.io/tag/python.html">python</a>,      <a href="http://littlezz.github.io/tag/xiao-jie.html">小结</a>    </p>
  </header>
  <div>
      <p><a href="https://github.com/littlezz/ESL-Model">ESLModel</a> 开发有一段时间了， 期间和numpy打了非常多的交道， 虐得我死去活来。<br>
</p>
<p>于是抽空写一下关于numpy的总结吧。<br>
</p>
<h2>Intro</h2>
<p>介绍一些会混淆的东西</p>
<h3>vector vs matrix</h3>
<p>vector是向量， 是一维的。 
matrix是二维以上的。<br>
</p>
<div class="highlight"><pre><span></span><span class="n">vec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># method 1</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">]])</span>

<span class="c1"># method 2</span>
<span class="n">mat</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># mathod 3</span>
<span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># output (3,)</span>

<span class="k">print</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># output (3, 1)</span>
</pre></div>


<ol>
<li>向量可以点乘矩阵， 反过来也可以， 结果是向量。</li>
<li>向量可以点乘向量， 得到标量</li>
<li>向量没有倒置</li>
<li>矩阵使用下标的时候， 结果是向量， 使用slice 来查询的时候， 得到子矩阵。<br>
</li>
</ol>
<p>如果想要得到  单行的子矩阵<br>
</p>
<div class="highlight"><pre><span></span><span class="n">mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">16</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="c1"># 得到一行， 矩阵</span>
<span class="n">mat</span><span class="p">[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>

<span class="c1"># 得到一列， 矩阵</span>
<span class="n">mat</span><span class="p">[:,[</span><span class="mi">1</span><span class="p">]]</span>

<span class="c1"># 得到第二列， 向量</span>
<span class="n">mat</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>

<span class="c1"># 子矩阵</span>
<span class="n">mat</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>

<span class="c1"># 下标查询</span>
<span class="n">mat</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
</pre></div>


<h3>axis=1 vs axis=0</h3>
<p>之前看官方的文档里面有说， 你要这样记：axis=0 是从上往下， axis=1 是从左往右处理。<br>
</p>
<p>比如<code>np.sum(mat, axis=0)</code> 得到每一列从上到下的累加和. <code>axis=1</code> 的时候得到每一行的累加和.<br>
</p>
<div class="highlight"><pre><span></span><span class="n">m2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">m2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">Out</span><span class="p">[</span><span class="mi">65</span><span class="p">]:</span>
<span class="n">array</span><span class="p">([</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">38</span><span class="p">])</span>
</pre></div>


<h4>rowvar=1</h4>
<p>在使用<code>np.cov</code> 的时候，里面有这个参数， 默认是非零， 表示你的矩阵是（p x N）的， 每一行表示变量， 每一列里面存的是样本数据。  如果<code>rowvar=0</code>, 则表示反过来.<br>
</p>
<h2>Tips</h2>
<h3>reshape</h3>
<p>reshape 里面的值设置-1的时候表示自动</p>
<div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>


<p>会自动变成(3x4)<br>
</p>
<h3>如何插入一列</h3>
<div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">12</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="c1"># 在0的开始插入全为3的列</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<h3>如何把向量转成单列矩阵</h3>
<div class="highlight"><pre><span></span><span class="c1"># column matrix</span>
<span class="n">vec</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span>

<span class="c1"># row matrix</span>
<span class="n">vec</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
</pre></div>


<h3>如何算矩阵的行列式$|A|$</h3>
<p>用<code>np.linalg.det</code> 或者 <code>np.linalg.slogdet</code>
后者在矩阵里面的值很小的时候使用, 得到的是log后的结果.</p>
<h3>ddof</h3>
<p>在<code>np.std</code>等函数中有<code>ddof</code>这个参数， 默认为0， 在pandas里面默认为1， 主要用于控制比如计算标准差的时候， 是除以N-1还是除以N， 这里相当于除以（N - ddof）</p>
<h3>svd</h3>
<div class="highlight"><pre><span></span><span class="n">U</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">D</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Vt</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># output(4, 4) (3,) (3, 3) (4, 3)</span>

<span class="c1"># with full_matrices=False</span>
<span class="n">U</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">D</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Vt</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># output (4, 3) (3,) (3, 3) (4, 3)</span>
</pre></div>


<p>注意到D是向量， Vt是V的转置矩阵。
多数情况下， 应该是使用full_matrices=False</p>
<h3>随机整数矩阵</h3>
<div class="highlight"><pre><span></span><span class="c1"># 随机生成包含0-9的 4x3 的矩阵</span>
<span class="n">d</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
</pre></div>


<h3>随机生成正交矩阵（orthogonal matrix）</h3>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_orthogonal_matrix</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    with random permutation of coordinate axes</span>
<span class="sd">    :param N: dimension of matrix</span>
<span class="sd">    :return: NxN orthogonal matrix contains 0 and 1</span>

<span class="sd">    Ref: http://stackoverflow.com/questions/33003341/how-to-randomly-generate-a-nonnegative-orthogonal-matrix-in-numpy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">I</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
</pre></div>


<h3>特征分解（eigen decomposition）</h3>
<p><code>np.eigh</code> 针对对称的矩阵， <strong>注意， eigenvalues 是升序的，而在R语言里面是降序的！</strong><br>
<code>np.eig</code> 普通的分解， eigenvalues不保证顺序。<br>
</p>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/python-package-setup.html#python-package-setup">Python Package Setup</a></h2>
    <p>
      Posted on Thu 17 March 2016 in <a href="http://littlezz.github.io/category/python.html">Python</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/python.html">python</a>,      <a href="http://littlezz.github.io/tag/xiao-jie.html">小结</a>    </p>
  </header>
  <div>
      <p>esl_model <a href="https://github.com/littlezz/ESL-Model">https://github.com/littlezz/ESL-Model</a><br>
今天算是弄完了第三章的代码， 放到github上面， 折腾python的打包折腾了大半天。<br>
</p>
<p>简单总结一下Python的Package Distribution吧。<br>
有空的话， 在写一个pytest的简单总结。</p>
<h2>setup.py</h2>
<p>setup.py 是最主要的整个配置的文件， </p>
<p>贴我的setup.py 的代码好了。<br>
</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">setuptools</span> <span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">find_packages</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;README.rst&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">readme</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">version</span> <span class="o">=</span> <span class="nb">__import__</span><span class="p">(</span><span class="s1">&#39;esl_model&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">__version__</span>

<span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;esl_model&#39;</span><span class="p">,</span>
      <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span>
      <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Algorithm from The Elements of Statistical Learning book implement by Python code&quot;</span><span class="p">,</span>
      <span class="n">long_description</span><span class="o">=</span><span class="n">readme</span><span class="p">,</span>
      <span class="n">classifiers</span><span class="o">=</span><span class="p">[</span>
          <span class="s1">&#39;Programming Language :: Python :: 3.5&#39;</span><span class="p">,</span>
          <span class="s1">&#39;Programming Language :: Python :: 3 :: Only&#39;</span><span class="p">,</span>
          <span class="s1">&#39;Development Status :: 2 - Pre-Alpha&#39;</span><span class="p">,</span>
          <span class="s1">&#39;License :: OSI Approved :: MIT License&#39;</span><span class="p">,</span>
      <span class="p">],</span>
      <span class="n">url</span><span class="o">=</span><span class="s1">&#39;https://github.com/littlezz/ESL-Model&#39;</span><span class="p">,</span>
      <span class="n">author</span><span class="o">=</span><span class="s1">&#39;littlezz&#39;</span><span class="p">,</span>
      <span class="n">author_email</span><span class="o">=</span><span class="s1">&#39;zz.at.field@gmail.com&#39;</span><span class="p">,</span>
      <span class="n">license</span><span class="o">=</span><span class="s1">&#39;MIT&#39;</span><span class="p">,</span>
      <span class="n">packages</span><span class="o">=</span><span class="n">find_packages</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;tests*&#39;</span><span class="p">]),</span>
      <span class="n">install_requires</span><span class="o">=</span><span class="p">[</span>
            <span class="s1">&#39;numpy&#39;</span><span class="p">,</span>
            <span class="s1">&#39;pandas&#39;</span><span class="p">,</span>
            <span class="s1">&#39;scipy&#39;</span><span class="p">,</span>
            <span class="s1">&#39;sklearn&#39;</span><span class="p">,</span>
      <span class="p">],</span>
      <span class="n">tests_require</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pytest&#39;</span><span class="p">],</span>
      <span class="n">include_package_data</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">zip_safe</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>


<p>逐个解释参数。<br>
<code>name</code> 是你要发布的模块的名字, 只能是用小写和下划线.不要和其他的公共的模块名字冲突.<br>
</p>
<p><code>version</code> 是版本号, 诸如 0.0.1 这样.<br>
</p>
<p><code>description</code> 和 <code>long_description</code>是你这个项目的介绍，用于pipy上面的介绍，注意 long_description 只能使用rst格式， <strong>不支持</strong>markdown。 <br>
</p>
<p><code>classifiers</code>是你在pipy上面的分类， 一般在这里要只能你是否支持py2， 或py3， 或者是否有特定的平台限制。<a href="https://pypi.python.org/pypi?%3Aaction=list_classifiers">所有classifiers 列表</a><br>
</p>
<p><code>packages</code>, 要包含的代码的文件夹。比如我也可以设置成 <code>esl_model</code>， 但是据我观察， 这样设置之后， 我下面的子模块是没有办法使用<code>import esl_model.ch3</code> 这样载入的。<br>
所以使用find_packages自动寻找所有模块， 特别排除我的测试文件存放的文件夹。<br>
</p>
<p><code>install_requires</code>, 依赖， 如果在虚拟环境里面的话可以用<code>pip freeze</code>来查看安装的库， 但是这里面会包含真正用到的库（比如里面可能会显示ipython）。 可以使用 pipreqs 这个库。<br>
</p>
<p><code>tests_require</code> 这个是测试的用的, 在<code>python setup.py test</code>的时候用到, 然而我偷懒了, pytest在这里还差一步配置， 所以这里有没有都一样。。<br>
</p>
<p><code>include_package_data</code>, 这个是包含非python代码的文件， 我的程序里面要使用到csv数据， 所以需要这个选项。<br>
</p>
<p>我的代码结构树</p>
<div class="highlight"><pre><span></span>── LICENSE.md
├── MANIFEST.in
├── README.rst
├── esl_model
│   ├── __init__.py
│   ├── __pycache__
│   ├── base.py
│   ├── ch3
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   └── model.py
│   ├── datasets
│   │   ├── __init__.py
│   │   ├── __pycache__
│   │   ├── base.py
│   │   └── data
│   │       └── prostate.csv
│   └── utils.py
├── pytest.ini
├── setup.py
└── tests
    ├── __init__.py
    ├── __pycache__
    ├── test_ch3.py
    └── utils.py
</pre></div>


<h2>MANIFEST.in</h2>
<p>指定需要包含的非程序文件<br>
</p>
<p>一般是<code>include README.rst</code> <br>
我这里因为有额外的数据， 所以要多一行。<br>
</p>
<div class="highlight"><pre><span></span>include README.rst
recursive-include esl_model/datasets/data *.csv
</pre></div>


<p>recursive-include 必须， 不然数据不包含。<br>
</p>
<h2>测试安装</h2>
<p><code>pip install -e .</code><br>
本地的调试安装模式。<br>
</p>
<h2>ref</h2>
<ul>
<li>sklearn的配置文件 <a href="https://github.com/scikit-learn/scikit-learn">https://github.com/scikit-learn/scikit-learn</a><br>
</li>
<li>Python-packaging <a href="https://python-packaging.readthedocs.org/en/latest/testing.html">https://python-packaging.readthedocs.org/en/latest/testing.html</a></li>
</ul>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/a-fa-gou-shao-nian-yi-ji-jin-yong.html#a-fa-gou-shao-nian-yi-ji-jin-yong">阿发狗，少年以及金庸</a></h2>
    <p>
      Posted on Tue 15 March 2016 in <a href="http://littlezz.github.io/category/diary.html">diary</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/xia-che.html">瞎扯</a>,      <a href="http://littlezz.github.io/tag/ri-ji.html">日记</a>    </p>
  </header>
  <div>
      <p>阿发狗3：0打赢了李世石， 整个世界沉浸在一股人工智能毁灭人类的狂欢中， 之后李世石翻了一盘。 众人笑道：AI已经学会放水啦， 药丸药丸， 真是可怕。<br>
</p>
<p>时至围棋界被阿发狗颠覆之际， 世界围棋第一人站了出来：就算阿发狗能打败李世石， 也打败不了我！大家定睛一看， 发现围棋第一人是个19岁少年， 于是纷纷以长辈自称， 要教他一下如何做人谦虚的人生道理。<br>
</p>
<p>围棋大师吴清源的脑残粉金庸同学3月10日九十二岁啦， 大家有什么想对他说的， 截止2016年03月16日00:44:36， 这个问题下面排名第一的答案写了一个小故事， 大意是金庸因为写了一篇暗讽教务主任的文章被其看到后居然就被退学了， 之后金庸都不提这段高中历史， 作者给于这件事情的评价是， 『傲娇』得很的金庸。末尾不忘用一个感叹号来增强语气。<br>
</p>
<p>我想起了周杰伦在《三年二班》里面唱到那句重复了好多遍的歌词<br>
</p>
<p>——<br>
</p>
<blockquote>
<p>不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 不好笑 </p>
</blockquote>
<p>这三件事情， 一点也不好笑。<br>
</p>
<p>如果你也曾在出发时志在必得， 结果却被打得落花流水， 咬紧牙关取得一些成绩却于事无补。<br>
</p>
<p>如果你也曾在同辈愉快玩耍的时候承受超越『年龄段』的磨练， 以此获得相应的能力， 却还是要被年长而无能者打着人生经验丰富的旗号『教育』一番。<br>
</p>
<p>如果你也曾空怀与学校教学目标不相符的能力， 而濒临退学， 抑或被直接退学。<br>
</p>
<p>那么， 你就会知道， 一点也不好笑。<br>
</p>
<p></br></p>
<p>三面的三件事， 其实都可以围绕围棋展开， 第一件事李世石困兽之斗， 基本上线上和线下的人都开玩笑说是AI放水， 对之前不可一世的李世石同学的嘲讽程度可见一斑。<br>
第二件事是19岁少年的『惊人』发言， 于是大家想要教他当大人。</p>
<p>然而这些举动其实在我们的吴清源大师面前， 其实都是小儿科。<br>
不过， 大家也不关心吴清源是谁是不是？<br>
</p>
<p>算了， 本来我还有半肚子的苦水没有得喷出来， 写到这里的时候也没有什么想说的了， 以前老是奇怪， 你说那些动物长得都一样的， 他们怎么分得清楚谁是谁？<br>
</p>
<p>我们分不清，但是他们分得清啊。 现在想来网络上也是一样的。<br>
隔着屏幕， 谁知道后面码字的谁是谁， 在这里被恶心完之后， 接着被下一个地方恶心， 我们总用一个『傻x』来标记他们， 结果发现这个世界上怎么这么多『傻x』， 然而仔细一想， 这个词后面是无数的人， 你干掉了一个， 又冒出一个，无穷尽，辩不赢， 赢不长久， 赢了也没用。最后哪怕是呕心沥血的赢了一次，花了时间和精力， 最后得到对面甘拜下风， 低头认输， 五体投地，结果过了不久又冒出一个。<br>
</p>
<p>所以当我发现我要写一些幼稚得不行的所谓的人生道理的时候， 我及时的把自己打住了。<br>
</p>
<p>有非常多的话要说， 但是在写作的时候， 我已经在自己的内心里面说了一遍， 这就够了。<br>
</p>
<p>如果有过一些相似的经历， 那么有些话是不必多说的， 如果没有过切身体会， 那么又何必多说？<br>
</p>
<p>2016年03月16日01:58:35<br>
</p>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/linear-modelde-zong-jie.html#linear-modelde-zong-jie">Linear Model的总结</a></h2>
    <p>
      Posted on Thu 10 March 2016 in <a href="http://littlezz.github.io/category/machine-learning.html">Machine-Learning</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/ji-qi-xue-xi.html">机器学习</a>    </p>
  </header>
  <div>
      <h2>目录</h2>
<ul>
<li>公式</li>
<li>易混淆的名词解释</li>
<li>蛋疼的程序编写过程<br>
</li>
</ul>
<h3>公式</h3>
<p>The elements of statistical learning 看到了第三章， 昨天着手于编写程序。<br>
</p>
<p>Linear Model: </p>
<p>$\hat\beta = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y$</p>
<p>在这里面， $\mathbf X$ 先要要standardization, 然后在第一列插入全为1的列， 用于截距计算(intercept)。<br>
所以beta hat 算出来后第一项为截距。<br>
</p>
<p>$\hat {\mathbf y} = \mathbf X \hat \beta$ </p>
<p>对数据做预测的时候， <strong>X也要standardization</strong>， 做和你之前一样的事情， 使用和<strong>train set一样的 mean 和 std</strong>， 说多了都是泪啊， 我昨天晚上一个晚上都在弄这个， 因为我的要预测的x用了自己的mean 和std， 结果出来的结果不一样。  一定要使用相同的std 和mean 来处理数据。 我才想起来ng的课里面写程序的时候， pdf里面还特别提示过， 妈的， 哭晕。<br>
</p>
<p>$var(\hat\beta)=(\mathbf X^T \mathbf X)^{-1}\sigma^2$<br>
</p>
<p>$\hat \sigma = \frac{1}{N-p-1} \sum_{i=1}^N(y_i - \hat y_i)^2$</p>
<p>然后是z-score<br>
$z_j = \frac{\hat\beta_j}{\hat \sigma \sqrt v_j}$</p>
<p>注意到$v_j$是$\mathbf X^T \mathbf X$的对角线上的第j项。<br>
所以得到stand error这里的做法是对$var(\hat\beta)$对角线开平方。</p>
<p>F statistic<br>
</p>
<p>$F = \frac{(RSS_0 - RSS_1)/(p_1 - p_0)}{RSS1 / (N - p_1 - 1)}$</p>
<p>F statistic 用于分析去掉一些feature之后影响大不大。
$RSS_1$ 是含有跟多feature的模型的代价， $RSS_0$是去掉一些feature之后的模型的代价。<br>
</p>
<h3>易混淆的名词解释</h3>
<h5>standardization</h5>
<p>这个的意识是让数据的feature 的平均值为0, 然后高斯分布为1.<br>
</p>
<p>针对的是每一个feature, 一般是<code>x = (x-x.mean(axis=0)) / x.std(asix=0, ddof=1)</code></p>
<p>默认ddof=0， 这个时候标准差（std） 是除以N的， 在机器学习里面， 一般是除以N， 但是在这本书里面， 一般是认为std的计算要除以（N-1）。<br>
</p>
<h5>normalize</h5>
<p>这个是针对每一个样例的， 比如X里面包含N组数据， 那么就是对N组数据做， 一般是把不同特征的取值范围降到同一个范围里面。<br>
</p>
<h3>蛋疼的程序编写过程</h3>
<p>我花了一天， 才发现， standardization对结果是不会影响的， 是一种优化方法。<br>
但是我因为对 Test_X 的数据standardization错了, 所以结果总是不一样. 非常的尴尬.<br>
</p>
<p>翻了sklearn的源码, 发现他里面怎么对y做了处理, 做了<code>(y-y.mean())</code>东西， 卧槽， 不合理啊。<br>
</p>
<p>昨天写了一天， 值写了linear Regression， 后面还有一堆， 慢慢写吧。<br>
</p>
<p>等稳定一些之后再放到github上面好了。<br>
</p>
<p>2016年03月10日15:12:10</p>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/gai-lu-lun-xue-xi.html#gai-lu-lun-xue-xi">概率论学习</a></h2>
    <p>
      Posted on Fri 29 January 2016 in <a href="http://littlezz.github.io/category/machine-learning.html">Machine-Learning</a>
    </p>
  </header>
  <div>
      <p>所用公式都是用mathjax 写的， ( ~~奶奶的， 写死我了。~~ )<br>
</p>
<h2>基本概念</h2>
<p>基本概念主要记下常用公式。</p>
<ul>
<li>P(A) 表示事件A发生的概率<br>
</li>
<li>$P(A \bigcup B) = P(A) + P(B) - P(AB)$<br>
</li>
<li>上面公式可以推广到N个事件<br>
</li>
<li>$P(B \mid A)  = \frac{P(AB)}{P(A)}$<br>
</li>
<li>$P(A) = P(AB)+P(A \bar B) = P(B) \cdot P(A|B) + P(\bar B) \cdot P(A| \bar B)$<br>
</li>
</ul>
<h5>全概率公式</h5>
<p>$B_i$ 是$S$的完备事件组<br>
</p>
<p>$P(A) = \sum_{i=1}^n P(B_i)P(A|B_i)$ <br>
</p>
<h5>贝叶斯公式</h5>
<p>$P(B_k|A) = {P(B_k A) \over P(A)} = {P(B_k)P(A|P(B_k) \over \sum_{j=1}^n P(B_j)P(A|B_j)}$</p>
<h2>概率分布</h2>
<h4>0-1分布</h4>
<p>$X \sim 0 - 1$<br>
</p>
<h4>二项分布</h4>
<p>$P{X= k} = C_n^kp^k(1-p)^k$<br>
</p>
<p>记作 $X \sim B(n,p)$</p>
<h4>poisson分布</h4>
<p>$P{X= k} = {e^{-\lambda}\lambda^k \over k!}$<br>
</p>
<p>记作 $X \sim \pi(\lambda)$</p>
<h3>Distribution function</h3>
<p>$F(x) = P{X \le x}$<br>
</p>
<h3>probability density function</h3>
<p>$f(x)$ 是概率密度函数<br>
满足<br>
</p>
<p>$P{X \in D} = \int_{D} f(x) {dx}$<br>
</p>
<h3>分布</h3>
<h4>均匀分布</h4>
<p>f(x) = 1/(b-a),  x在（a,b)内， 其他的地方为0<br>
</p>
<p>$X \sim U(a,b)$<br>
</p>
<h4>正态分布 （normal）</h4>
<p>$X \sim N(\mu, \sigma^2)$<br>
</p>
<p>性质<br>
- f(x) 关于 $x=\mu$ 对称<br>
- $\sigma$越大, 最大值越大, 就是越瘦高.  <br>
- N(0, 1) 是标准正态分布<br>
</p>
<h4>指数分布</h4>
<p>$f(x) = \lambda e^{-\lambda x}$, x&gt;0<br>
</p>
<p>记作$X \sim E(\lambda)$</p>
<h4>others</h4>
<p>比如 Gamma分布， 
$X \sim \Gamma(\alpha, \beta)$</p>
<p>Beta 分布<br>
$X \sim \beta(a, b)$</p>
  </div>
  <hr />
</article>
<article>
  <header>
    <h2><a href="http://littlezz.github.io/yao-wan-zhen-de-shi-yao-wan.html#yao-wan-zhen-de-shi-yao-wan">药丸， 真的是药丸</a></h2>
    <p>
      Posted on Wed 27 January 2016 in <a href="http://littlezz.github.io/category/diary.html">diary</a>
      &#8226; Tagged with
      <a href="http://littlezz.github.io/tag/li-ren-ge.html">里人格</a>,      <a href="http://littlezz.github.io/tag/ri-ji.html">日记</a>,      <a href="http://littlezz.github.io/tag/xia-che.html">瞎扯</a>    </p>
  </header>
  <div>
      <p>今天浪了一天， 不懂在干什么， 本来说要补一波概率论的， 结果看到第4章之后就开始乱翻书了， 然后统计学习基础那本书根本就没有看， 昨天看到『前列腺癌』的例子， 今天还是这个例子。 我很关心男性前列腺癌嘛啊？<br>
</p>
<p>感觉真的药丸。<br>
</p>
<p>昨天和手榴弹去吃KFC， 作死的点了一个全家桶， 嘛， 作为肥宅， kfc当然要刷全家桶啦。<br>
</p>
<p>结果就是吃得快吐了， 肥宅力不足嘛。<br>
人家A岛上的肥宅都是单刷全家桶， 结果我们两个人也搞不完一个， 主要是汉堡， 简直是想象不出的难吃， 等吃到汉堡的时候， 进入嘴巴的东西基本已经没有什么味道了， 可是汉堡的那个面包， 用蛋蛋的话来说叫做『面饼』， （里面夹着的东西叫做『肉饼』）， 真是难吃， 难吃， 难吃。<br>
</p>
<p>我偷偷留了两口没有吃完， 藏在汉堡盒子里面， 结果在战斗结束拍纪念照的时候被我弄了出来， 蛋蛋惊呼：『卧槽， 你作弊！居然还藏了半个面饼！』<br>
</p>
<p>纪念照是以冷漠的肥宅为主题， 通过拍摄在成堆的K记包装盒于凌乱的可乐杯后面低头玩手机的男人， 表现肥宅刷完全家桶事后玩手机的冷漠之情。<br>
</p>
<p><img alt="肥宅" src="../images/IMG_0593.jpg"><br>
</p>
<p>饭后， 天早已经全黑，走在回去的路上， 我早已直不起腰， 就算是还剩10秒的绿灯也只能放弃， 停下来等待。<br>
</p>
<p>世界一下子陷入了缓慢的行动之中。  在这样的节奏之中， 却还是能够超过那些男女成群嘻嘻哈哈的路人，<br>
</p>
<p>在一个十字路口之前， 蛋蛋提到了毕业， 我连能不能毕业都不知道， 这学期估计要吃退学警告咯， 我哈哈的回应道。<br>
</p>
<p>蛋蛋自己也挂了两科， 而且是考试全部都会， 但是因为没有平时分， 结果就被老师给挂了。<br>
</p>
<p>操， 大不了就延毕呗。<br>
</p>
<p>不知道为何这句压在我心口的话语经他之口说出来之后， 我一下子轻松了非常多。 </p>
<p>人流在十字路口前面聚集， 在红绿灯的结束时间通过是基本规则， 没有人在乎你是不是撑得快死了。<br>
</p>
<p>在下一个红绿灯再过马路有什么问题？<br>
如果不过红绿灯会怎么样？<br>
</p>
<p>我不知道到。<br>
</p>
<p>他们说， 山崩地裂， 日月无光。<br>
他们说， 你本来有一个美好的前途。<br>
他们说， 你完蛋定了啊。<br>
</p>
<p>说完这些之后， 继续投身于和小贩对青菜能不能少一毛钱的战斗之中。<br>
</p>
<p>我看着远处的猪肉摊， 肉头堆积成山， 屠户把猪头奋力的塞进黑色的袋子里面， 对太大的头， 用刀切成两半， 手起刀落， 骨肉四溅。 </p>
<p>在那一刻， 他是否有感受到， 大千世界曾由他主宰的快感？<br>
</p>
<p>在那一刻， 他们是不是也因此产生了参透世界，看破轮回的感觉， 于是有了对他人做出预言的底气。<br>
</p>
<p>我不知道如何回应， 我也不知道路的那头有什么， 途中会有弹射起步的跑车路过么？ 会中出一架大卡车么？<br>
</p>
<p>这个学期， 加入的公司没有融到钱， 团队解散了， 但是我好歹也达成了筹够去日本剁手的钱的目的。 <br>
</p>
<p>干了一个学期， 目的是达成了， 自己也开始有了新的作死计划， 开始看起了机器学习， 两年前， 我还在嚷嚷着， 写程序要什么数学？<br>
两年后， 我看着那些书中中出的各种的公式， 简直就像是被中出一样。<br>
</p>
<p>摸摸自己的额头， 擦了把汗， 我果然还是想要试一下啊。<br>
于是Ng的课算是上完了， 你要问我记住了什么， 说一句实话， 对不起，什么也没有记住。<br>
</p>
<p>毕不了业， 药丸， 毕得了业， 之后等着的也是找工作， 下一个十字路口，药丸。 找到了工作， 浑水摸鱼， 虚度光阴， 药丸。  摸滚打爬， 事业有成， 娶不到老婆， 药丸。  取到了老婆， 以前吃的化学添加剂太多， 生不出小孩， 药丸。  小孩成绩不好， 药丸。<br>
事业有成， 取了老婆， 生了小孩， 头发掉光了， 药丸。<br>
腰酸背痛， 药丸。<br>
前列腺炎， 药丸。<br>
老眼昏花， 药丸。<br>
年事已高， 大小便失禁， 卧床不起， 病房外的小孩因为一个发烧感冒要打针吃药就大喊大叫的， 看着自己浑身插满的管子， <br>
这一次没有人说你药丸了。<br>
</p>
<p>但是你我都知道，这下是真的要完了。<br>
</p>
<p>弗洛伊德分析法， 我们总是药丸。<br>
不管做什么， 只要『他们』想， 你都是药丸。<br>
</p>
<p>和蛋蛋在十字路口道别， 我转身没入小区的夜色之中， 在回去的路上， 我已经放弃思考『为什么 <strong>他们</strong> 总是赢？』这样无解的问题了。   <br>
蛋蛋说， 赶紧回去， 我要去杀僵尸了， 一斧子砍爆僵尸的头。 特别爽。 <br>
我也赶着回去捞新船。<br>
</p>
<blockquote>
<p>让一让, 母牛们, 人生短暂啊!<br>
</p>
</blockquote>
<p>当他的奶牛们疯狂繁殖， 每天开着宴会， 结果吃得差点撑死。<br>
当他真的需要钱给女儿上学的时候， 奶牛却不在繁殖， 失去了上帝眷顾的又老又聋的他和情人挨家挨户的卖自己徒手制作的小彩票。  最后病死。</p>
<p>随着年龄的增长和阅历的增加， 我越来越觉得这个世界的疯狂程度，有过之而无不及。<br>
</p>
<p>教授问我， 啊？你怎么现在就出去工作了？ 我干笑。<br>
公司的运营总监问我， 你还没有毕业啊？你的工作就是给CTO打杂的么？  我笑笑。<br>
</p>
<p>面对『他们』发出的这种疑问， 这种暴露自己早已没有任何想象力的疑问， 以及发生的很多事情， 挂科也好， 作为差生中的差生也好， 其实我他妈是不服的。<br>
我没有那种生来就上天入地扭曲现实的能力，但是却也不认为是被上帝抛弃的人，  <br>
</p>
<p>或许， 这就是普通吧。<br>
</p>
<p>普通的焦虑着。  <br>
普通的痛苦着。 <br>
普通的挣扎着。<br>
</p>
<p>正因为普通， 所以为了不沦落成『他们』， 不得不经历不普通的事情， 为了在以后， 能遇上和自己一样挣扎的普通人。<br>
</p>
<p>说说那天接下来的事情吧， 之后我回家就接着玩船去了， 吃得撑了，加上玩了3个小时， 而且有差不多一个小时都在刷同一个图（1-5），精神已经恍惚了， 结果大破进击了。<br>
</p>
<p>我之前有一点犹豫， 迷迷糊糊的，不确定到底有没有大破， 当选完阵型， 看到我大破的小学生的时候， 我整个人一下就蒙蔽了， 吓出了一身冷汗， 『卧槽， 我要粪提了』， 赶紧F5， 重进游戏之后看到自己的小学生还在。 松了一口气， 事后大佬告诉我在选阵型之前F5才有效， 这次存粹是运气好吧了。<br>
</p>
<p>差点药丸， 运气好而已， 但是如果真的沉船了， 可能我也会和基友倾诉一下， 药丸药丸，然后接着玩下去。<br>
</p>
<p>药丸。<br>
药丸。<br>
</p>
<p>虽然各种药丸， 一不小心就药丸。但是还是得走下去， 因为， <br>
</p>
<p>当我们大喊着药丸的时候，你我都知道， 这还不是真正的 <strong>要完</strong> </p>
<p>2016年01月28日03:32:37</p>
  </div>
</article>

  <div class="pagination">
    <a class="btn" href="http://littlezz.github.io/index3.html">
      <i class="fa fa-angle-left"></i> Older Posts
    </a>
    <a class="btn float-right" href="http://littlezz.github.io/index.html">
      Newer Posts <i class="fa fa-angle-right"></i>
    </a>
  </div>

    <footer>
        <p>&copy; littlezz 2017</p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " littlezz's Blog ",
  "url" : "http://littlezz.github.io",
  "image": "/images/logo.jpg",
  "description": "littlezz's Thoughts and Writings About Python And Machine Learning and diary"
}
</script></body>
</html>