Title: How to Write Reduced Rank Linear Discriminant Analysis with Python   
Tags: machine learning, 机器学习, python, 小结, numpy  

Intro
-------
I'm working on a project named [ESL-Model](https://github.com/littlezz/ESL-Model) which I want to use python to implent algorithm from The Elements of Statistical Learning. I start writing RRLDA the day before yesterday,  and I sccuessfully finish until today.  

Notice that I am using Python3.5, which allow me use `@` instead of `np.dot`  


The difference between Numpy and R
----------------------------------
Firstly, Numpy is slight different with R in eigen decomposition(I spent lots of time to find out T_T).  

In R, the eigenvalues return from `eigen` is descending, which is all notes assume.  
But Numpy `np.linalg.eigh` return eigenvalues in ascending order.  

It is doesn't matter when you work on LDA, because it use both $V$ and $D$, but it influence the result in Reduced rank LDA which you use column of $V$ alone.  

To slove this problem, you may use `np.fliplr`.  

```python
W= np.array([[ 0.9967,  0.002 ],
       [ 0.002 ,  1.0263]])

Dw, Uw = np.linalg.eigh(W)
Dw = Dw[::-1]
Uw = np.fliplr(Uw)
```

You can get 

```
print(Dw)
[ 1.02643452  0.99656548]


print(Dw)
[[ 0.06711024 -0.99774557]
 [ 0.99774557  0.06711024]]
```

How to Write Reduced Rank LDA
------------
Finally, we arrive at the algorithm.  
I mainly reference http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/lda2.pdf.   
Which is a very very nice ppt, thanks it very much!  


- Find the centroids for all the classes and class prior probabilities, which is the same in LDA ($\mu_k$ and $\pi_k$).  

- Find between-class covariance matrix B using the centroid vectors and class prior probabilities  
    suppose $\mu_k$ is column vector

    $\mu = \Sigma_1^K \mu_k$  
    $B =  \Sigma_1^K \pi_k(\mu - \mu_k)(\mu - \mu_k)^T$
      
      
    **Notice that the code dosen't take same shape with formula**  
    
    ```python
    W = self.Sigma_hat
    # prior probabilities (K,1)
    Pi = self.Pi
    # class centroids (K, p)
    Mu = self.Mu
    p = self.p
    # the number of class
    K = self.K
    # the dimension you want
    L = self.L

    # Mu is (K,p) matrix, Pi is (K,1)
    mu = np.sum(Pi * Mu, axis=0)
    B = np.zeros((p, p))

    for k in range(K):
        # vector @ vector equal scalar, use vector[:, None] to transform to matrix
        # vec[:, None] equal to vec.reshape((1, vec.shape[0]))
        B = B + Pi[k]*((Mu[k] - mu)[:, None] @ ((Mu[k] - mu)[None, :]))
    ```
    
- Find within-class covariance matrix W, the same with $\hat \Sigma$ in LDA.  

- eigen decomposition W

    $W = V_WD_WV_W^T$  
    Define $W^{1/2} = D_W^{1/2}V_W^T$
    
    So $W^{-\frac{1}{2}} = (W^{1/2})^{-1}$  
    
    
    ```python
    # Be careful, the `eigh` method get the eigenvalues in ascending , which is opposite to R.
    Dw, Uw = LA.eigh(W)
    # reverse the Dw_ and Uw
    Dw = Dw[::-1]
    Uw = np.fliplr(Uw)

    W_half = np.linalg.pinv(np.diagflat(Dw**0.5) @ Uw.T)
    ```
    
- compute $B^*$  
    $B^* = (W^{-\frac{1}{2}})^TBW^{-\frac{1}{2}}$  
    
    ```python  
    B_star = W_half.T @ B @ W_half
    
    ```
    
- eigen decompostion $B^*$, get $a_l$  
    $B^* = VDV^T$  
    $a_l = W^{-\frac{1}{2}}V_l$
    
    ```python  
    D_, V = LA.eigh(B_star)

    # reverse V
    V = np.fliplr(V)
    
    A = np.zeros((L, p))
    for l in range(L):
        A[l, :] = (W_half) @ V[:, l]
    ```
    
- transform X and $\mu_k$ and get predict value
    $\hat x = Ax$  
    $\hat \mu_k = A\mu_k$
    
    find $argmin_k {|\hat x- \hat\mu|_2^2 - log(\pi_k)}$
    
    ```python
    X_star = X @ A.T

    for k in range(self.K):
        # mu_s_star shape is (p,)
        mu_k_star = A @ self.Mu[k]

        # Ref: http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html
        # Ref: http://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy
        Y[:, k] = LA.norm(X_star - mu_k_star, axis=1) * 0.5 - log(self.Pi[k])
        
    # Python index start from 0, transform to start with 1
    y_hat = Y.argmin(axis=1).reshape((-1, 1)) + 1
    ```
    
Put it Together
---------------
You can find complete code in here
https://github.com/littlezz/ESL-Model/blob/master/esl_model/ch4/model.py

Reference
-----------
- The Elements of Statistical Learning 2nd Edition section 4.3.2 - 4.3.3  
- http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/lda2.pdf  
- https://onlinecourses.science.psu.edu/stat857/node/83
- http://www.stat.cmu.edu/~ryantibs/datamining/lectures/21-clas2-marked.pdf




2016-03-26 15:43:45
