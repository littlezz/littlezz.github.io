<!DOCTYPE html>
<html lang="en">
<head>
  <!-- <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'> -->
   <link href='//fonts.proxy.ustclug.org/css?family=Source+Sans+Pro:300,400,700,400italic' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/style.min.css">
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/pygments.min.css">
  <link rel="stylesheet" type="text/css" href="http://littlezz.github.io/theme/css/font-awesome.min.css">
  <link href="http://littlezz.github.io/static/custom.css" rel="stylesheet">
  <link href="http://littlezz.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="littlezz's Blog Atom">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />
<meta name="author" content="littlezz" />
<meta name="description" content="总结一下第10周， 之前看的都没有总结（←懒 第十周已经开始不怎么说算法了， 这次说的是对于大数据集的处理， 之前那种每次都对所有数据集做计算的称为batch， 现在这个（stochastic）是每次只对一个样例找最优值， 遍历完所有样例之后也会趋向于全局最优， 会在他的旁边震荡， 这个时候可以动态的减少学习效率 (alpha) 之后说了mini-batch， 就是stochastic 和batch的混合版， 每次只训练一小部分样例。 然后在线的学习其实感觉就是stochastic， 每次都对一个新的样本训练， 然后丢弃。 之后是map reduce， 这个没有什么好说的了， 分布式计算， 或者并行的多核计算， 分布式要考虑网络带宽等问题， 虽然视频说多核的不用考虑带宽， 但是实际上核心之间的通信也要消耗时间， 最终可能每个核心只能快80%吧。 map reduce的使用需要计算的时候数据能够分割， 一般是有 Σ 这种求和公式的时候使用。 嘛。 2016年01月24日14:42:29" />
<meta name="keywords" content="机器学习">
<!--<meta property="og:site_name" content="littlezz's Blog"/>
<meta property="og:title" content="Ng Machine Learning Week 10"/>
<meta property="og:description" content="总结一下第10周， 之前看的都没有总结（←懒 第十周已经开始不怎么说算法了， 这次说的是对于大数据集的处理， 之前那种每次都对所有数据集做计算的称为batch， 现在这个（stochastic）是每次只对一个样例找最优值， 遍历完所有样例之后也会趋向于全局最优， 会在他的旁边震荡， 这个时候可以动态的减少学习效率 (alpha) 之后说了mini-batch， 就是stochastic 和batch的混合版， 每次只训练一小部分样例。 然后在线的学习其实感觉就是stochastic， 每次都对一个新的样本训练， 然后丢弃。 之后是map reduce， 这个没有什么好说的了， 分布式计算， 或者并行的多核计算， 分布式要考虑网络带宽等问题， 虽然视频说多核的不用考虑带宽， 但是实际上核心之间的通信也要消耗时间， 最终可能每个核心只能快80%吧。 map reduce的使用需要计算的时候数据能够分割， 一般是有 Σ 这种求和公式的时候使用。 嘛。 2016年01月24日14:42:29"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="http://littlezz.github.io/ng-machine-learning-week-10.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2016-01-24 14:49:33+08:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="http://littlezz.github.io/author/littlezz.html">
<meta property="article:section" content="Machine-Learning"/>
<meta property="article:tag" content="机器学习"/>
<meta property="og:image" content="/images/logo.jpg">-->
  <title>littlezz's Blog &ndash; Ng Machine Learning Week 10</title>
</head>
<body>
  <aside>
    <div>
      <a href="http://littlezz.github.io">
        <img src="/images/logo.jpg" alt="littlezz" title="littlezz">
      </a>
      <h1><a href="http://littlezz.github.io">littlezz</a></h1>
      <p>Everything that has a beginning has an end.</p>
      <nav>
        <ul class="list">
        </ul>
      </nav>
      <ul class="social">
        <li><a class="sc-github" href="https://github.com/littlezz/" target="_blank"><i class="fa fa-github"></i></a></li>
      </ul>
    </div>
  </aside>
  <main>
    <nav>
      <a href="http://littlezz.github.io">Home</a>
      <a href="/categories">Category</a>
      <a href="/tags">Tags</a>
      <a href="http://littlezz.github.io/feeds/all.atom.xml">Atom</a>
    </nav>

<article>
  <header>
    <h1 id="ng-machine-learning-week-10">Ng Machine Learning Week 10</h1>
    <p>Posted on Sun 24 January 2016 in <a href="http://littlezz.github.io/category/machine-learning.html">Machine-Learning</a></p>
  </header>
  <div>
    <p>总结一下第10周， 之前看的都没有总结（←懒   </p>
<p>第十周已经开始不怎么说算法了， 这次说的是对于大数据集的处理， 之前那种每次都对所有数据集做计算的称为batch， 现在这个（stochastic）是每次只对一个样例找最优值， 遍历完所有样例之后也会趋向于全局最优， 会在他的旁边震荡， 这个时候可以动态的减少学习效率 (alpha)   </p>
<p>之后说了mini-batch， 就是stochastic 和batch的混合版， 每次只训练一小部分样例。</p>
<p>然后在线的学习其实感觉就是stochastic， 每次都对一个新的样本训练， 然后丢弃。  </p>
<p>之后是map reduce， 这个没有什么好说的了， 分布式计算， 或者并行的多核计算， 分布式要考虑网络带宽等问题， 虽然视频说多核的不用考虑带宽， 但是实际上核心之间的通信也要消耗时间， 最终可能每个核心只能快80%吧。  </p>
<p>map reduce的使用需要计算的时候数据能够分割， 一般是有 Σ 这种求和公式的时候使用。  </p>
<p>嘛。  </p>
<p>2016年01月24日14:42:29</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="http://littlezz.github.io/tag/ji-qi-xue-xi.html">机器学习</a>
    </p>
  </div>
</article>

    <footer>
        <p>&copy; littlezz 2017</p>
<p>Built using <a href="http://getpelican.com" target="_blank">Pelican</a> - <a href="https://github.com/alexandrevicenzi/flex" target="_blank">Flex</a> theme by <a href="http://alexandrevicenzi.com" target="_blank">Alexandre Vicenzi</a></p>    </footer>
  </main>





<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "name": "Ng Machine Learning Week 10",
  "headline": "Ng Machine Learning Week 10",
  "datePublished": "2016-01-24 14:49:33+08:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "littlezz",
    "url": "http://littlezz.github.io/author/littlezz.html"
  },
  "image": "/images/logo.jpg",
  "url": "http://littlezz.github.io/ng-machine-learning-week-10.html",
  "description": "总结一下第10周， 之前看的都没有总结（←懒 第十周已经开始不怎么说算法了， 这次说的是对于大数据集的处理， 之前那种每次都对所有数据集做计算的称为batch， 现在这个（stochastic）是每次只对一个样例找最优值， 遍历完所有样例之后也会趋向于全局最优， 会在他的旁边震荡， 这个时候可以动态的减少学习效率 (alpha) 之后说了mini-batch， 就是stochastic 和batch的混合版， 每次只训练一小部分样例。 然后在线的学习其实感觉就是stochastic， 每次都对一个新的样本训练， 然后丢弃。 之后是map reduce， 这个没有什么好说的了， 分布式计算， 或者并行的多核计算， 分布式要考虑网络带宽等问题， 虽然视频说多核的不用考虑带宽， 但是实际上核心之间的通信也要消耗时间， 最终可能每个核心只能快80%吧。 map reduce的使用需要计算的时候数据能够分割， 一般是有 Σ 这种求和公式的时候使用。 嘛。 2016年01月24日14:42:29"
}
</script></body>
</html>