<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>littlezz's Blog</title><link href="http://littlezz.github.io/" rel="alternate"></link><link href="http://littlezz.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>http://littlezz.github.io/</id><updated>2016-03-10T00:00:00+08:00</updated><entry><title>Linear Model的总结</title><link href="http://littlezz.github.io/linear-modelde-zong-jie.html" rel="alternate"></link><updated>2016-03-10T00:00:00+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-03-10:linear-modelde-zong-jie.html</id><summary type="html">&lt;h2 id="_1"&gt;&lt;a name="user-content-_1" href="#_1" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;公式&lt;/li&gt;
&lt;li&gt;易混淆的名词解释&lt;/li&gt;
&lt;li&gt;蛋疼的程序编写过程  &lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_2"&gt;&lt;a name="user-content-_2" href="#_2" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;公式&lt;/h3&gt;
&lt;p&gt;The elements of statistical learning 看到了第三章， 昨天着手于编写程序。  &lt;/p&gt;
&lt;p&gt;Linear Model: &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\hat\beta = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在这里面， &lt;span class="math"&gt;\(\mathbf X\)&lt;/span&gt; 先要要standardization, 然后在第一列插入全为1的列， 用于截距计算(intercept)。&lt;br /&gt;
所以beta hat 算出来后第一项为截距。  &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\hat {\mathbf y} = \mathbf X \hat \beta\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;对数据做预测的时候， &lt;strong&gt;X也要standardization&lt;/strong&gt;， 做和你之前一样的事情， 使用和**train set一样的 mean 和 std**， 说多了都是泪啊， 我昨天晚上一个晚上都在弄这个， 因为我的要预测的x用了自己的mean 和std， 结果出来的结果不一样。  一定要使用相同的std 和mean 来处理数据。 我才想起来ng的课里面写程序的时候， pdf里面还特别提示过， 妈的， 哭晕。  &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(var(\hat\beta)=(\mathbf X^T \mathbf X)^{-1}\sigma^2\)&lt;/span&gt;  &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\hat \sigma = \frac{1}{N-p-1} \sum_{i=1}^N(y_i - \hat y_i)^2\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;然后是z-score&lt;br /&gt;
&lt;span class="math"&gt;\(z_j = \frac{\hat\beta_j}{\hat \sigma \sqrt v_j}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意到&lt;span class="math"&gt;\(v_j\)&lt;/span&gt;是&lt;span class="math"&gt;\(\mathbf X^T \mathbf X\)&lt;/span&gt;的对角线上的第j项。&lt;br /&gt;
所以得到stand error这里的做法是对&lt;span class="math"&gt;\(var(\hat\beta)\)&lt;/span&gt;对角线开平方。&lt;/p&gt;
&lt;p&gt;F statistic  &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(F = \frac{(RSS_0 - RSS_1)/(p_1 - p_0)}{RSS1 / (N - p_1 - 1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;F statistic 用于分析去掉一些feature之后影响大不大。&lt;br /&gt;
&lt;span class="math"&gt;\(RSS_1\)&lt;/span&gt; 是含有跟多feature的模型的代价， &lt;span class="math"&gt;\(RSS_0\)&lt;/span&gt;是去掉一些feature之后的模型的代价。  &lt;/p&gt;
&lt;h3 id="_3"&gt;&lt;a name="user-content-_3" href="#_3" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;易混淆的名词解释&lt;/h3&gt;
&lt;h5 id="standardization"&gt;&lt;a name="user-content-standardization" href="#standardization" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;standardization&lt;/h5&gt;
&lt;p&gt;这个的意识是让数据的feature 的平均值为0, 然后高斯分布为1.  &lt;/p&gt;
&lt;p&gt;针对的是每一个feature, 一般是&lt;code&gt;x = (x-x.mean(axis=0)) / x.std(asix=0, ddof=1)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;默认ddof=0， 这个时候标准差（std） 是除以N的， 在机器学习里面， 一般是除以N， 但是在这本书里面， 一般是认为std的计算要除以（N-1）。  &lt;/p&gt;
&lt;h5 id="normalize"&gt;&lt;a name="user-content-normalize" href="#normalize" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;normalize&lt;/h5&gt;
&lt;p&gt;这个是针对每一个样例的， 比如X里面包含N组数据， 那么就是对N组数据做， 一般是把不同特征的取值范围降到同一个范围里面。  &lt;/p&gt;
&lt;h3 id="_4"&gt;&lt;a name="user-content-_4" href="#_4" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;蛋疼的程序编写过程&lt;/h3&gt;
&lt;p&gt;我花了一天， 才发现， standardization对结果是不会影响的， 是一种优化方法。&lt;br /&gt;
但是我因为对 Test_X 的数据standardization错了, 所以结果总是不一样. 非常的尴尬.  &lt;/p&gt;
&lt;p&gt;翻了sklearn的源码, 发现他里面怎么对y做了处理, 做了&lt;code&gt;(y-y.mean())&lt;/code&gt;东西， 卧槽， 不合理啊。  &lt;/p&gt;
&lt;p&gt;昨天写了一天， 值写了linear Regression， 后面还有一堆， 慢慢写吧。  &lt;/p&gt;
&lt;p&gt;等稳定一些之后再放到github上面好了。  &lt;/p&gt;
&lt;p&gt;2016年03月10日15:12:10&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "false";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><category term="机器学习"></category></entry><entry><title>概率论学习</title><link href="http://littlezz.github.io/gai-lu-lun-xue-xi.html" rel="alternate"></link><updated>2016-01-29T00:00:00+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-01-29:gai-lu-lun-xue-xi.html</id><summary type="html">&lt;p&gt;所用公式都是用mathjax 写的， ( &lt;del&gt;奶奶的， 写死我了。&lt;/del&gt; )  &lt;/p&gt;
&lt;h2 id="_1"&gt;&lt;a name="user-content-_1" href="#_1" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;基本概念&lt;/h2&gt;
&lt;p&gt;基本概念主要记下常用公式。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(A) 表示事件A发生的概率  &lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(A \bigcup B) = P(A) + P(B) - P(AB)\)&lt;/span&gt;  &lt;/li&gt;
&lt;li&gt;上面公式可以推广到N个事件  &lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(B \mid A)  = \frac{P(AB)}{P(A)}\)&lt;/span&gt;  &lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(P(A) = P(AB)+P(A \bar B) = P(B) \cdot P(A|B) + P(\bar B) \cdot P(A| \bar B)\)&lt;/span&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id="_2"&gt;&lt;a name="user-content-_2" href="#_2" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;全概率公式&lt;/h5&gt;
&lt;p&gt;&lt;span class="math"&gt;\(B_i\)&lt;/span&gt; 是&lt;span class="math"&gt;\(S\)&lt;/span&gt;的完备事件组  &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(A) = \sum_{i=1}^n P(B_i)P(A|B_i)\)&lt;/span&gt;   &lt;/p&gt;
&lt;h5 id="_3"&gt;&lt;a name="user-content-_3" href="#_3" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;贝叶斯公式&lt;/h5&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P(B_k|A) = {P(B_k A) \over P(A)} = {P(B_k)P(A|P(B_k) \over \sum_{j=1}^n P(B_j)P(A|B_j)}\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="_4"&gt;&lt;a name="user-content-_4" href="#_4" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;概率分布&lt;/h2&gt;
&lt;h4 id="0-1"&gt;&lt;a name="user-content-0-1" href="#0-1" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;0-1分布&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X \sim 0 - 1\)&lt;/span&gt;  &lt;/p&gt;
&lt;h4 id="_5"&gt;&lt;a name="user-content-_5" href="#_5" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;二项分布&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P\{X= k\} = C_n^kp^k(1-p)^k\)&lt;/span&gt;  &lt;/p&gt;
&lt;p&gt;记作 &lt;span class="math"&gt;\(X \sim B(n,p)\)&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="poisson"&gt;&lt;a name="user-content-poisson" href="#poisson" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;poisson分布&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P\{X= k\} = {e^{-\lambda}\lambda^k \over k!}\)&lt;/span&gt;  &lt;/p&gt;
&lt;p&gt;记作 &lt;span class="math"&gt;\(X \sim \pi(\lambda)\)&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="distribution-function"&gt;&lt;a name="user-content-distribution-function" href="#distribution-function" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;Distribution function&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(F(x) = P\{X \le x\}\)&lt;/span&gt;  &lt;/p&gt;
&lt;h3 id="probability-density-function"&gt;&lt;a name="user-content-probability-density-function" href="#probability-density-function" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;probability density function&lt;/h3&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; 是概率密度函数&lt;br /&gt;
满足  &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P\{X \in D\} = \int_{D} f(x) {dx}\)&lt;/span&gt;  &lt;/p&gt;
&lt;h3 id="_6"&gt;&lt;a name="user-content-_6" href="#_6" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;分布&lt;/h3&gt;
&lt;h4 id="_7"&gt;&lt;a name="user-content-_7" href="#_7" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;均匀分布&lt;/h4&gt;
&lt;p&gt;f(x) = 1/(b-a),  x在（a,b)内， 其他的地方为0  &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X \sim U(a,b)\)&lt;/span&gt;  &lt;/p&gt;
&lt;h4 id="normal"&gt;&lt;a name="user-content-normal" href="#normal" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;正态分布 （normal）&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(X \sim N(\mu, \sigma^2)\)&lt;/span&gt;  &lt;/p&gt;
&lt;p&gt;性质&lt;br /&gt;
- f(x) 关于 &lt;span class="math"&gt;\(x=\mu\)&lt;/span&gt; 对称&lt;br /&gt;
- &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;越大, 最大值越大, 就是越瘦高.  &lt;br /&gt;
- N(0, 1) 是标准正态分布  &lt;/p&gt;
&lt;h4 id="_8"&gt;&lt;a name="user-content-_8" href="#_8" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;指数分布&lt;/h4&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f(x) = \lambda e^{-\lambda x}\)&lt;/span&gt;, x&amp;gt;0  &lt;/p&gt;
&lt;p&gt;记作&lt;span class="math"&gt;\(X \sim E(\lambda)\)&lt;/span&gt;&lt;/p&gt;
&lt;h4 id="others"&gt;&lt;a name="user-content-others" href="#others" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;others&lt;/h4&gt;
&lt;p&gt;比如 Gamma分布， &lt;br /&gt;
&lt;span class="math"&gt;\(X \sim \Gamma(\alpha, \beta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Beta 分布&lt;br /&gt;
&lt;span class="math"&gt;\(X \sim \beta(a, b)\)&lt;/span&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "false";

    if (true) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary></entry><entry><title>Ng Machine Learning Week 11</title><link href="http://littlezz.github.io/ng-machine-learning-week-11.html" rel="alternate"></link><updated>2016-01-24T20:04:20+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-01-24:ng-machine-learning-week-11.html</id><summary type="html">&lt;p&gt;第十一周说的是处理图片， 介绍了滑动窗口， 如何获得（制造）更多的数据， 还有在pipeline 中的上限分析， 这个是用来决定在系统中应该跟加关心那些模块的优化， 哪些模块就算优化了效果也比较小。  当然了， 喜闻乐见的黑了一下『我认识的人』， 花了18个月去优化一个没有太大效果的模块。  &lt;/p&gt;
&lt;h4 id="_1"&gt;&lt;a name="user-content-_1" href="#_1" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;滑动窗口&lt;/h4&gt;
&lt;p&gt;图片处理的时候是训练识别一小块， 然后滑动窗口来在大图中寻找是否找到。 比如你的样本都是 10x10 的大小， 然后等比例缩放， 在原图中你也可以找 50x50 的, 但是对于这个范围， 你得缩小到10x10， 然后在来识别。  &lt;/p&gt;
&lt;h4 id="_2"&gt;&lt;a name="user-content-_2" href="#_2" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;展开&lt;/h4&gt;
&lt;p&gt;里面还顺带提及了， 什么展开（好吧我不记得具体的名字了）， 在识别图像里面的文字的时候， 首先要找到包含文字的区域， 比如标记为白色， 然后这些白色向周围扩散， 比如20个像素点， 如果这之中也有其他的文字区域（即白色）， 着把他们连成一块大的白色区域。&lt;/p&gt;
&lt;h4 id="_3"&gt;&lt;a name="user-content-_3" href="#_3" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;制造数据&lt;/h4&gt;
&lt;p&gt;用现有样本， 加上噪声之类的， 得到新的训练数据。&lt;br /&gt;
他建议在解决问题前， 先算一下如果我要得到10倍的训练数据， 我需要多大的代价， 比如人工10秒做一次判断， 那么如果我有新的10k条数据， 可能就是人工工作几天， 但是我得到了更加多的数据， 也就是说， 我的训练结果得到了一个巨大的提升。   &lt;/p&gt;
&lt;h4 id="_4"&gt;&lt;a name="user-content-_4" href="#_4" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;上限分析&lt;/h4&gt;
&lt;p&gt;先要模块化， 然后重头开始给于一个模块绝对正确的预测结果给下一个模块， 计算其中系统正确率的变化。  &lt;/p&gt;
&lt;p&gt;然后就可以明确的知道哪些模块不需要花精力啦（&lt;del&gt;前提是你得弄成pipeline， 一般自己写的话都是大杂烩吧&lt;/del&gt;）&lt;/p&gt;
&lt;p&gt;同时不忘黑那个18个月的哥们，&lt;del&gt;这家伙我好想在第一周的时候感觉就听说过他了&lt;/del&gt;。&lt;/p&gt;
&lt;h4 id="_5"&gt;&lt;a name="user-content-_5" href="#_5" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;终结&lt;/h4&gt;
&lt;p&gt;最后的最后， Andrew 结束词的时候， 我非常的感动（才没有哭！）， 这是我大学之后， 上过的第一门真正体现『传道， 授业， 解惑』的课。&lt;del&gt;（大学上的那个不叫课， 叫吃屎）&lt;/del&gt; &lt;/p&gt;
&lt;p&gt;我还记得第三周的时候， 介绍各种优化， 他坦然的承认自己搞了10年对于这些优化也是最近才渐渐明白里面的原理， 我隔着屏幕， 跨越时空， 哈哈哈的笑着。  &lt;/p&gt;
&lt;p&gt;我还记得， 当我们上完线性回归和logic回归的时候， 他说， 你们现在已经比大多数硅谷工作的工程师更加懂机器学习了， 他们中有很多人靠机器学习赚到了许多钱， 或者帮助公司取得成功。 然后露出一个诙谐的笑容时候， 我也跟着笑着， 这家伙， 整天黑硅谷的人。  &lt;/p&gt;
&lt;p&gt;我还记得当他鼓励完我们， 接着告诉我们， 他还有更厉害的东西要教授我们的时候， 我真的觉得接下来要得到巨大力量， 倍感兴奋。 &lt;/p&gt;
&lt;p&gt;就在最后， 他说， 同学们， 谢谢你们， 我知道你们当中很多人花了大量的时间的来学习， 同时花了大量的时间做哪些长度很长的编程题目，希望你们能在自己的领域利用机器学习取得成功， 希望你们能够让更多的人生活得更美好的时候。  &lt;/p&gt;
&lt;p&gt;我觉得这个祝福一点也不畸形， 我觉得他有资格对他的学生们说希望你们改变世界， 当他说出这句话的时候， 我没有丝毫的觉得矫揉造作， 而是觉得， 理应如此。&lt;/p&gt;
&lt;p&gt;因为，&lt;br /&gt;
师者，传道授业解惑者也。  &lt;/p&gt;
&lt;p&gt;我知道我已经严重偏题了， 这些东西都应该归类到个人日记那一块， 而且我也知道反正也没有人看，但是我还是想在一个属于『公开』的地方， 写下此时此刻的想法。  &lt;/p&gt;
&lt;p&gt;我觉得人的能力真的不是最重要的， 最重要的是他对这个世界， 对其他的人，对周边的事物还存有的一份爱意， 还相信自己在任何情况下都存在着可能性。&lt;br /&gt;
这些才是最重要的， 那些所谓的能力， 终究是科学训练的产物， 这个世界对于我们真正的考验， 是科学之外的东西。  &lt;/p&gt;
&lt;p&gt;所以， 爱因斯坦说， &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一个人在智力上的成就很大程度上取决于人格的伟大，这一点往往超出人们通常的认识。  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因为，我们还有很长的路要走。&lt;/p&gt;
&lt;p&gt;以上。  &lt;/p&gt;
&lt;p&gt;2016年01月24日21:29:36&lt;/p&gt;</summary><category term="机器学习"></category><category term="瞎扯"></category></entry><entry><title>Ng Machine Learning Week 10</title><link href="http://littlezz.github.io/ng-machine-learning-week-10.html" rel="alternate"></link><updated>2016-01-24T14:49:33+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-01-24:ng-machine-learning-week-10.html</id><summary type="html">&lt;p&gt;总结一下第10周， 之前看的都没有总结（←懒   &lt;/p&gt;
&lt;p&gt;第十周已经开始不怎么说算法了， 这次说的是对于大数据集的处理， 之前那种每次都对所有数据集做计算的称为batch， 现在这个（stochastic）是每次只对一个样例找最优值， 遍历完所有样例之后也会趋向于全局最优， 会在他的旁边震荡， 这个时候可以动态的减少学习效率 (alpha)   &lt;/p&gt;
&lt;p&gt;之后说了mini-batch， 就是stochastic 和batch的混合版， 每次只训练一小部分样例。&lt;/p&gt;
&lt;p&gt;然后在线的学习其实感觉就是stochastic， 每次都对一个新的样本训练， 然后丢弃。  &lt;/p&gt;
&lt;p&gt;之后是map reduce， 这个没有什么好说的了， 分布式计算， 或者并行的多核计算， 分布式要考虑网络带宽等问题， 虽然视频说多核的不用考虑带宽， 但是实际上核心之间的通信也要消耗时间， 最终可能每个核心只能快80%吧。  &lt;/p&gt;
&lt;p&gt;map reduce的使用需要计算的时候数据能够分割， 一般是有 Σ 这种求和公式的时候使用。  &lt;/p&gt;
&lt;p&gt;嘛。  &lt;/p&gt;
&lt;p&gt;2016年01月24日14:42:29&lt;/p&gt;</summary><category term="机器学习"></category></entry><entry><title>ML-ex8</title><link href="http://littlezz.github.io/ml-ex8.html" rel="alternate"></link><updated>2016-01-24T00:00:00+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-01-24:ml-ex8.html</id><summary type="html">&lt;p&gt;今天撸了Ng的机器学习的第九周的作业。 课之前看了， 作业拖到我学校的考试周结束了才做。 诶一提起这个就想到我挂科估计要挂到退学警告了。。。  &lt;/p&gt;
&lt;p&gt;这一周的是关于异常检查和推荐系统的， 协同过滤我之前看过诶， 和这个不一样， Ng还是求那个gradient 什么鬼的， 可是我在另一篇&lt;a href="http://www.salemmarafi.com/code/collaborative-filtering-with-python/"&gt;文章&lt;/a&gt;里面看到的是另一个东西。&lt;/p&gt;
&lt;p&gt;他也是用movie作为行， 用户作为列， 而且是用户直接当做了feature， 然后算两个movie之间的距离， 不过这样说来， 好像和机器学习就没有关系了。。。  &lt;/p&gt;
&lt;p&gt;另外， 每次做作业的感觉其实就是， 我不会机器学习， 我只是公式的搬运工....  T_T  &lt;/p&gt;
&lt;p&gt;说一下搬运工的心得吧。&lt;br /&gt;
今天把公式转换成矩阵的向量运算的时候折腾了好久。  &lt;/p&gt;
&lt;p&gt;后来我还是没有发现什么取巧的办法， google了一下也没有找到好的技巧（←其实就找了一下）， 总结起来， 还是pdf里面说的  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先用for 循环&lt;/li&gt;
&lt;li&gt;2维的可以直接matrix&lt;/li&gt;
&lt;li&gt;3维带一个for loop&lt;/li&gt;
&lt;li&gt;&lt;del&gt;一般矩阵相乘形式上正确结果就是正确的&lt;/del&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实我花了半个小时在想如何消除3层嵌套的for循环。  /(ㄒoㄒ)/~~  &lt;/p&gt;
&lt;p&gt;嘛， 接着看吧。（&lt;del&gt;弱逼不敢乱说话&lt;/del&gt;）  &lt;/p&gt;
&lt;p&gt;2016年01月24日00:57:49&lt;/p&gt;</summary><category term="机器学习"></category></entry></feed>