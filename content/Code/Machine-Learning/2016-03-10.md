Title: Linear Model的总结  
Tags: 机器学习  


目录
-------
- 公式
- 易混淆的名词解释
- 蛋疼的程序编写过程  

###公式

The elements of statistical learning 看到了第三章， 昨天着手于编写程序。  

Linear Model: 

$\hat\beta = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y$

在这里面， $\mathbf X$ 先要要standardization, 然后在第一列插入全为1的列， 用于截距计算(intercept)。  
所以beta hat 算出来后第一项为截距。  

 
$\hat {\mathbf y} = \mathbf X \hat \beta$ 

对数据做预测的时候， **X也要standardization**， 做和你之前一样的事情， 使用和**train set一样的 mean 和 std**， 说多了都是泪啊， 我昨天晚上一个晚上都在弄这个， 因为我的要预测的x用了自己的mean 和std， 结果出来的结果不一样。  一定要使用相同的std 和mean 来处理数据。 我才想起来ng的课里面写程序的时候， pdf里面还特别提示过， 妈的， 哭晕。  



$var(\hat\beta)=(\mathbf X^T \mathbf X)^{-1}\sigma^2$  

$\hat \sigma = \frac{1}{N-p-1} \sum_{i=1}^N(y_i - \hat y_i)^2$

然后是z-score  
$z_j = \frac{\hat\beta_j}{\hat \sigma \sqrt v_j}$

注意到$v_j$是$\mathbf X^T \mathbf X$的对角线上的第j项。  
所以得到stand error这里的做法是对$var(\hat\beta)$对角线开平方。

F statistic  

$F = \frac{(RSS_0 - RSS_1)/(p_1 - p_0)}{RSS1 / (N - p_1 - 1)}$

F statistic 用于分析去掉一些feature之后影响大不大。
$RSS_1$ 是含有跟多feature的模型的代价， $RSS_0$是去掉一些feature之后的模型的代价。  


###易混淆的名词解释

#####standardization  
这个的意识是让数据的feature 的平均值为0, 然后高斯分布为1.  

针对的是每一个feature, 一般是`x = (x-x.mean(axis=0)) / x.std(asix=0, ddof=1)`

默认ddof=0， 这个时候标准差（std） 是除以N的， 在机器学习里面， 一般是除以N， 但是在这本书里面， 一般是认为std的计算要除以（N-1）。  

#####normalize  
这个是针对每一个样例的， 比如X里面包含N组数据， 那么就是对N组数据做， 一般是把不同特征的取值范围降到同一个范围里面。  



###蛋疼的程序编写过程
我花了一天， 才发现， standardization对结果是不会影响的， 是一种优化方法。  
但是我因为对 Test_X 的数据standardization错了, 所以结果总是不一样. 非常的尴尬.  

翻了sklearn的源码, 发现他里面怎么对y做了处理, 做了`(y-y.mean())`东西， 卧槽， 不合理啊。  

昨天写了一天， 值写了linear Regression， 后面还有一堆， 慢慢写吧。  

等稳定一些之后再放到github上面好了。  

2016年03月10日15:12:10



