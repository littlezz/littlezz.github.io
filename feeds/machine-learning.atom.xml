<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>littlezz's Blog - Machine-Learning</title><link href="http://littlezz.github.io/" rel="alternate"></link><link href="http://littlezz.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>http://littlezz.github.io/</id><updated>2016-03-26T00:00:00+08:00</updated><entry><title>How to Write Reduced Rank Linear Discriminant Analysis with Python</title><link href="http://littlezz.github.io/how-to-write-reduced-rank-linear-discriminant-analysis-with-python.html" rel="alternate"></link><published>2016-03-26T00:00:00+08:00</published><updated>2016-03-26T00:00:00+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-03-26:/how-to-write-reduced-rank-linear-discriminant-analysis-with-python.html</id><summary type="html">&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;I'm working on a project named &lt;a href="https://github.com/littlezz/ESL-Model"&gt;ESL-Model&lt;/a&gt; which I want to use python to implent algorithm from The Elements of Statistical Learning. I start writing RRLDA the day before yesterday,  and I sccuessfully finish until today.&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;Notice that I am using Python3.5, which allow me use &lt;code&gt;@&lt;/code&gt; instead of &lt;code&gt;np.dot&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h2&gt;The difference between Numpy and R&lt;/h2&gt;
&lt;p&gt;Firstly, Numpy is slight different with R in eigen decomposition(I spent lots of time to find out T_T).&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;In R, the eigenvalues return from &lt;code&gt;eigen&lt;/code&gt; is descending, which is all notes …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;I'm working on a project named &lt;a href="https://github.com/littlezz/ESL-Model"&gt;ESL-Model&lt;/a&gt; which I want to use python to implent algorithm from The Elements of Statistical Learning. I start writing RRLDA the day before yesterday,  and I sccuessfully finish until today.&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;Notice that I am using Python3.5, which allow me use &lt;code&gt;@&lt;/code&gt; instead of &lt;code&gt;np.dot&lt;/code&gt;&lt;br&gt;
&lt;/p&gt;
&lt;h2&gt;The difference between Numpy and R&lt;/h2&gt;
&lt;p&gt;Firstly, Numpy is slight different with R in eigen decomposition(I spent lots of time to find out T_T).&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;In R, the eigenvalues return from &lt;code&gt;eigen&lt;/code&gt; is descending, which is all notes assume.&lt;br&gt;
But Numpy &lt;code&gt;np.linalg.eigh&lt;/code&gt; return eigenvalues in ascending order.&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;It is doesn't matter when you work on LDA, because it use both $V$ and $D$, but it influence the result in Reduced rank LDA which you use column of $V$ alone.&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;To slove this problem, you may use &lt;code&gt;np.fliplr&lt;/code&gt;.&lt;br&gt;
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt; &lt;span class="mf"&gt;0.9967&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;0.002&lt;/span&gt; &lt;span class="p"&gt;],&lt;/span&gt;
       &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="mf"&gt;0.002&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="mf"&gt;1.0263&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;

&lt;span class="n"&gt;Dw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Uw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eigh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;Dw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Dw&lt;/span&gt;&lt;span class="p"&gt;[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;Uw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fliplr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Uw&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can get &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(Dw)
[ 1.02643452  0.99656548]

print(Dw)
[[ 0.06711024 -0.99774557]
 [ 0.99774557  0.06711024]]
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;How to Write Reduced Rank LDA&lt;/h2&gt;
&lt;p&gt;Finally, we arrive at the algorithm.&lt;br&gt;
I mainly reference http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/lda2.pdf. &lt;br&gt;
Which is a very very nice ppt, thanks it very much!&lt;br&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Find the centroids for all the classes and class prior probabilities, which is the same in LDA ($\mu_k$ and $\pi_k$).&lt;br&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Find between-class covariance matrix B using the centroid vectors and class prior probabilities&lt;br&gt;
    suppose $\mu_k$ is column vector&lt;/p&gt;
&lt;p&gt;$\mu = \Sigma_1^K \mu_k$&lt;br&gt;
$B =  \Sigma_1^K \pi_k(\mu - \mu_k)(\mu - \mu_k)^T$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notice that the code dosen't take same shape with formula&lt;/strong&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;```python
W = self.Sigma_hat&lt;/p&gt;
&lt;h1&gt;prior probabilities (K,1)&lt;/h1&gt;
&lt;p&gt;Pi = self.Pi&lt;/p&gt;
&lt;h1&gt;class centroids (K, p)&lt;/h1&gt;
&lt;p&gt;Mu = self.Mu
p = self.p&lt;/p&gt;
&lt;h1&gt;the number of class&lt;/h1&gt;
&lt;p&gt;K = self.K&lt;/p&gt;
&lt;h1&gt;the dimension you want&lt;/h1&gt;
&lt;p&gt;L = self.L&lt;/p&gt;
&lt;h1&gt;Mu is (K,p) matrix, Pi is (K,1)&lt;/h1&gt;
&lt;p&gt;mu = np.sum(Pi * Mu, axis=0)
B = np.zeros((p, p))&lt;/p&gt;
&lt;p&gt;for k in range(K):
    # vector @ vector equal scalar, use vector[:, None] to transform to matrix
    # vec[:, None] equal to vec.reshape((1, vec.shape[0]))
    B = B + Pi[k]*((Mu[k] - mu)[:, None] @ ((Mu[k] - mu)[None, :]))
```&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Find within-class covariance matrix W, the same with $\hat \Sigma$ in LDA.&lt;br&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;eigen decomposition W&lt;/p&gt;
&lt;p&gt;$W = V_WD_WV_W^T$&lt;br&gt;
Define $W^{1/2} = D_W^{1/2}V_W^T$&lt;/p&gt;
&lt;p&gt;So $W^{-\frac{1}{2}} = (W^{1/2})^{-1}$&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;```python&lt;/p&gt;
&lt;h1&gt;Be careful, the &lt;code&gt;eigh&lt;/code&gt; method get the eigenvalues in ascending , which is opposite to R.&lt;/h1&gt;
&lt;p&gt;Dw, Uw = LA.eigh(W)&lt;/p&gt;
&lt;h1&gt;reverse the Dw_ and Uw&lt;/h1&gt;
&lt;p&gt;Dw = Dw[::-1]
Uw = np.fliplr(Uw)&lt;/p&gt;
&lt;p&gt;W_half = np.linalg.pinv(np.diagflat(Dw**0.5) @ Uw.T)
```&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;compute $B^&lt;em&gt;$&lt;br&gt;
    $B^&lt;/em&gt; = (W^{-\frac{1}{2}})^TBW^{-\frac{1}{2}}$&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;```python&lt;br&gt;
B_star = W_half.T @ B @ W_half&lt;/p&gt;
&lt;p&gt;```&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;eigen decompostion $B^&lt;em&gt;$, get $a_l$&lt;br&gt;
    $B^&lt;/em&gt; = VDV^T$&lt;br&gt;
    $a_l = W^{-\frac{1}{2}}V_l$&lt;/p&gt;
&lt;p&gt;```python&lt;br&gt;
D_, V = LA.eigh(B_star)&lt;/p&gt;
&lt;h1&gt;reverse V&lt;/h1&gt;
&lt;p&gt;V = np.fliplr(V)&lt;/p&gt;
&lt;p&gt;A = np.zeros((L, p))
for l in range(L):
    A[l, :] = (W_half) @ V[:, l]
```&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;transform X and $\mu_k$ and get predict value
    $\hat x = Ax$&lt;br&gt;
    $\hat \mu_k = A\mu_k$&lt;/p&gt;
&lt;p&gt;find $argmin_k {|\hat x- \hat\mu|_2^2 - log(\pi_k)}$&lt;/p&gt;
&lt;p&gt;```python
X_star = X @ A.T&lt;/p&gt;
&lt;p&gt;for k in range(self.K):
    # mu_s_star shape is (p,)
    mu_k_star = A @ self.Mu[k]&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Ref: http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html
# Ref: http://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy
Y[:, k] = LA.norm(X_star - mu_k_star, axis=1) * 0.5 - log(self.Pi[k])
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Python index start from 0, transform to start with 1&lt;/h1&gt;
&lt;p&gt;y_hat = Y.argmin(axis=1).reshape((-1, 1)) + 1
```&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Put it Together&lt;/h2&gt;
&lt;p&gt;You can find complete code in here
https://github.com/littlezz/ESL-Model/blob/master/esl_model/ch4/model.py&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The Elements of Statistical Learning 2nd Edition section 4.3.2 - 4.3.3&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;http://sites.stat.psu.edu/~jiali/course/stat597e/notes2/lda2.pdf&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;https://onlinecourses.science.psu.edu/stat857/node/83&lt;/li&gt;
&lt;li&gt;http://www.stat.cmu.edu/~ryantibs/datamining/lectures/21-clas2-marked.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2016-03-26 15:43:45&lt;/p&gt;</content><category term="machine learning"></category><category term="机器学习"></category><category term="python"></category><category term="小结"></category><category term="numpy"></category></entry><entry><title>Fuck the Reduced-rank LDA</title><link href="http://littlezz.github.io/fuck-the-reduced-rank-lda.html" rel="alternate"></link><published>2016-03-25T00:00:00+08:00</published><updated>2016-03-25T00:00:00+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-03-25:/fuck-the-reduced-rank-lda.html</id><summary type="html">&lt;p&gt;从昨天开始一直在弄RRLDA， 直到今天晚上， 无果。&lt;br&gt;
搜索各种资料， 各自有理， 到最后连计算 $W^{-\frac{1}{2}}$ 都能有不同的版本。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;权当整理思路吧， 整理一下各种版本（可能我编程有问题， 反正所有版本我都没能算对：）&lt;br&gt;
&lt;/p&gt;
&lt;h2&gt;版本一： ESL&lt;/h2&gt;
&lt;p&gt;page 114 页的算法。&lt;br&gt;
我翻译一下：&lt;br&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算 类的重心 K x P 的矩阵M， 还有公共协方差矩阵W；&lt;/li&gt;
&lt;li&gt;计算 $M^* = MW^{-\frac{1}{2}}$, 使用W的特征分解；&lt;/li&gt;
&lt;li&gt;计算$B^&lt;em&gt;$, $M^&lt;/em&gt;$的协方差矩阵（B是为了between-class）, 还有他的特征分解 $B^&lt;em&gt; = V^&lt;/em&gt;D_BV^{&lt;em&gt;T}$. $V^&lt;/em&gt;$的按顺序的列$v_l^*$定义了最佳子空间的坐标。&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;卧槽， 我他妈看懂了。&lt;br&gt;
我往下看了一行。 
书上的意思是说， 你本来要算这些jb玩意的， 但是后来有一个叫做『捕鱼者（fisher）』的捕鱼达人， 给出了更吊的东西， 卧槽， 我他妈真的咸鱼啊， 这个人一波收割啊卧槽。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;那我懂了， 我接着看书了。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;FUCK！&lt;br&gt;
FUCK ME！&lt;/p&gt;
&lt;p&gt;2016年03月25日00:46:12&lt;/p&gt;</summary><content type="html">&lt;p&gt;从昨天开始一直在弄RRLDA， 直到今天晚上， 无果。&lt;br&gt;
搜索各种资料， 各自有理， 到最后连计算 $W^{-\frac{1}{2}}$ 都能有不同的版本。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;权当整理思路吧， 整理一下各种版本（可能我编程有问题， 反正所有版本我都没能算对：）&lt;br&gt;
&lt;/p&gt;
&lt;h2&gt;版本一： ESL&lt;/h2&gt;
&lt;p&gt;page 114 页的算法。&lt;br&gt;
我翻译一下：&lt;br&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算 类的重心 K x P 的矩阵M， 还有公共协方差矩阵W；&lt;/li&gt;
&lt;li&gt;计算 $M^* = MW^{-\frac{1}{2}}$, 使用W的特征分解；&lt;/li&gt;
&lt;li&gt;计算$B^&lt;em&gt;$, $M^&lt;/em&gt;$的协方差矩阵（B是为了between-class）, 还有他的特征分解 $B^&lt;em&gt; = V^&lt;/em&gt;D_BV^{&lt;em&gt;T}$. $V^&lt;/em&gt;$的按顺序的列$v_l^*$定义了最佳子空间的坐标。&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;卧槽， 我他妈看懂了。&lt;br&gt;
我往下看了一行。 
书上的意思是说， 你本来要算这些jb玩意的， 但是后来有一个叫做『捕鱼者（fisher）』的捕鱼达人， 给出了更吊的东西， 卧槽， 我他妈真的咸鱼啊， 这个人一波收割啊卧槽。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;那我懂了， 我接着看书了。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;FUCK！&lt;br&gt;
FUCK ME！&lt;/p&gt;
&lt;p&gt;2016年03月25日00:46:12&lt;/p&gt;</content><category term="machine learning"></category><category term="机器学习"></category><category term="他妈的"></category><category term="学学学，学个屁"></category></entry><entry><title>Linear Model的总结</title><link href="http://littlezz.github.io/linear-modelde-zong-jie.html" rel="alternate"></link><published>2016-03-10T00:00:00+08:00</published><updated>2016-03-10T00:00:00+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-03-10:/linear-modelde-zong-jie.html</id><summary type="html">&lt;h2&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;公式&lt;/li&gt;
&lt;li&gt;易混淆的名词解释&lt;/li&gt;
&lt;li&gt;蛋疼的程序编写过程&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;公式&lt;/h3&gt;
&lt;p&gt;The elements of statistical learning 看到了第三章， 昨天着手于编写程序。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;Linear Model: &lt;/p&gt;
&lt;p&gt;$\hat\beta = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y$&lt;/p&gt;
&lt;p&gt;在这里面， $\mathbf X$ 先要要standardization, 然后在第一列插入全为1的列， 用于截距计算(intercept)。&lt;br&gt;
所以beta hat 算出来后第一项为截距。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$\hat {\mathbf y} = \mathbf X \hat \beta$ &lt;/p&gt;
&lt;p&gt;对数据做预测的时候， &lt;strong&gt;X也要standardization&lt;/strong&gt;， 做和你之前一样的事情， 使用和&lt;strong&gt;train set一样的 mean 和 std&lt;/strong&gt;， 说多了都是泪啊， 我昨天晚上一个晚上都在弄这个， 因为我的要预测的x用了自己的mean 和std， 结果出来的结果不一样。  一定要使用相同的std 和mean 来处理数据。 我才想起来ng的课里面写程序的时候， pdf里面还特别提示过， 妈的， 哭晕。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$var(\hat\beta)=(\mathbf X^T \mathbf X)^{-1}\sigma^2$&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$\hat \sigma = \frac{1}{N-p-1} \sum_{i=1}^N(y_i - \hat y_i)^2$&lt;/p&gt;
&lt;p&gt;然后是z-score …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;目录&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;公式&lt;/li&gt;
&lt;li&gt;易混淆的名词解释&lt;/li&gt;
&lt;li&gt;蛋疼的程序编写过程&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;公式&lt;/h3&gt;
&lt;p&gt;The elements of statistical learning 看到了第三章， 昨天着手于编写程序。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;Linear Model: &lt;/p&gt;
&lt;p&gt;$\hat\beta = (\mathbf X^T \mathbf X)^{-1} \mathbf X^T \mathbf y$&lt;/p&gt;
&lt;p&gt;在这里面， $\mathbf X$ 先要要standardization, 然后在第一列插入全为1的列， 用于截距计算(intercept)。&lt;br&gt;
所以beta hat 算出来后第一项为截距。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$\hat {\mathbf y} = \mathbf X \hat \beta$ &lt;/p&gt;
&lt;p&gt;对数据做预测的时候， &lt;strong&gt;X也要standardization&lt;/strong&gt;， 做和你之前一样的事情， 使用和&lt;strong&gt;train set一样的 mean 和 std&lt;/strong&gt;， 说多了都是泪啊， 我昨天晚上一个晚上都在弄这个， 因为我的要预测的x用了自己的mean 和std， 结果出来的结果不一样。  一定要使用相同的std 和mean 来处理数据。 我才想起来ng的课里面写程序的时候， pdf里面还特别提示过， 妈的， 哭晕。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$var(\hat\beta)=(\mathbf X^T \mathbf X)^{-1}\sigma^2$&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$\hat \sigma = \frac{1}{N-p-1} \sum_{i=1}^N(y_i - \hat y_i)^2$&lt;/p&gt;
&lt;p&gt;然后是z-score&lt;br&gt;
$z_j = \frac{\hat\beta_j}{\hat \sigma \sqrt v_j}$&lt;/p&gt;
&lt;p&gt;注意到$v_j$是$\mathbf X^T \mathbf X$的对角线上的第j项。&lt;br&gt;
所以得到stand error这里的做法是对$var(\hat\beta)$对角线开平方。&lt;/p&gt;
&lt;p&gt;F statistic&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$F = \frac{(RSS_0 - RSS_1)/(p_1 - p_0)}{RSS1 / (N - p_1 - 1)}$&lt;/p&gt;
&lt;p&gt;F statistic 用于分析去掉一些feature之后影响大不大。
$RSS_1$ 是含有跟多feature的模型的代价， $RSS_0$是去掉一些feature之后的模型的代价。&lt;br&gt;
&lt;/p&gt;
&lt;h3&gt;易混淆的名词解释&lt;/h3&gt;
&lt;h5&gt;standardization&lt;/h5&gt;
&lt;p&gt;这个的意识是让数据的feature 的平均值为0, 然后高斯分布为1.&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;针对的是每一个feature, 一般是&lt;code&gt;x = (x-x.mean(axis=0)) / x.std(asix=0, ddof=1)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;默认ddof=0， 这个时候标准差（std） 是除以N的， 在机器学习里面， 一般是除以N， 但是在这本书里面， 一般是认为std的计算要除以（N-1）。&lt;br&gt;
&lt;/p&gt;
&lt;h5&gt;normalize&lt;/h5&gt;
&lt;p&gt;这个是针对每一个样例的， 比如X里面包含N组数据， 那么就是对N组数据做， 一般是把不同特征的取值范围降到同一个范围里面。&lt;br&gt;
&lt;/p&gt;
&lt;h3&gt;蛋疼的程序编写过程&lt;/h3&gt;
&lt;p&gt;我花了一天， 才发现， standardization对结果是不会影响的， 是一种优化方法。&lt;br&gt;
但是我因为对 Test_X 的数据standardization错了, 所以结果总是不一样. 非常的尴尬.&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;翻了sklearn的源码, 发现他里面怎么对y做了处理, 做了&lt;code&gt;(y-y.mean())&lt;/code&gt;东西， 卧槽， 不合理啊。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;昨天写了一天， 值写了linear Regression， 后面还有一堆， 慢慢写吧。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;等稳定一些之后再放到github上面好了。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;2016年03月10日15:12:10&lt;/p&gt;</content><category term="机器学习"></category></entry><entry><title>概率论学习</title><link href="http://littlezz.github.io/gai-lu-lun-xue-xi.html" rel="alternate"></link><published>2016-01-29T00:00:00+08:00</published><updated>2016-01-29T00:00:00+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-01-29:/gai-lu-lun-xue-xi.html</id><summary type="html">&lt;p&gt;所用公式都是用mathjax 写的， ( ~~奶奶的， 写死我了。~~ )&lt;br&gt;
&lt;/p&gt;
&lt;h2&gt;基本概念&lt;/h2&gt;
&lt;p&gt;基本概念主要记下常用公式。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(A) 表示事件A发生的概率&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;$P(A \bigcup B) = P(A) + P(B) - P(AB)$&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;上面公式可以推广到N个事件&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;$P(B \mid A)  = \frac{P(AB)}{P(A)}$&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;$P(A) = P(AB)+P(A \bar B) = P(B) \cdot P(A|B) + P(\bar B) \cdot P(A| \bar B)$&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;全概率公式&lt;/h5&gt;
&lt;p&gt;$B_i$ 是$S$的完备事件组&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$P(A) = \sum_{i=1}^n P(B_i)P(A|B_i)$ &lt;br&gt;
&lt;/p&gt;
&lt;h5&gt;贝叶斯公式&lt;/h5&gt;
&lt;p&gt;$P(B_k|A) = {P(B_k A) \over P(A)} = {P(B_k)P(A|P(B_k) \over \sum_{j=1}^n P(B_j …&lt;/p&gt;</summary><content type="html">&lt;p&gt;所用公式都是用mathjax 写的， ( ~~奶奶的， 写死我了。~~ )&lt;br&gt;
&lt;/p&gt;
&lt;h2&gt;基本概念&lt;/h2&gt;
&lt;p&gt;基本概念主要记下常用公式。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P(A) 表示事件A发生的概率&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;$P(A \bigcup B) = P(A) + P(B) - P(AB)$&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;上面公式可以推广到N个事件&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;$P(B \mid A)  = \frac{P(AB)}{P(A)}$&lt;br&gt;
&lt;/li&gt;
&lt;li&gt;$P(A) = P(AB)+P(A \bar B) = P(B) \cdot P(A|B) + P(\bar B) \cdot P(A| \bar B)$&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;全概率公式&lt;/h5&gt;
&lt;p&gt;$B_i$ 是$S$的完备事件组&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$P(A) = \sum_{i=1}^n P(B_i)P(A|B_i)$ &lt;br&gt;
&lt;/p&gt;
&lt;h5&gt;贝叶斯公式&lt;/h5&gt;
&lt;p&gt;$P(B_k|A) = {P(B_k A) \over P(A)} = {P(B_k)P(A|P(B_k) \over \sum_{j=1}^n P(B_j)P(A|B_j)}$&lt;/p&gt;
&lt;h2&gt;概率分布&lt;/h2&gt;
&lt;h4&gt;0-1分布&lt;/h4&gt;
&lt;p&gt;$X \sim 0 - 1$&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;二项分布&lt;/h4&gt;
&lt;p&gt;$P{X= k} = C_n^kp^k(1-p)^k$&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;记作 $X \sim B(n,p)$&lt;/p&gt;
&lt;h4&gt;poisson分布&lt;/h4&gt;
&lt;p&gt;$P{X= k} = {e^{-\lambda}\lambda^k \over k!}$&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;记作 $X \sim \pi(\lambda)$&lt;/p&gt;
&lt;h3&gt;Distribution function&lt;/h3&gt;
&lt;p&gt;$F(x) = P{X \le x}$&lt;br&gt;
&lt;/p&gt;
&lt;h3&gt;probability density function&lt;/h3&gt;
&lt;p&gt;$f(x)$ 是概率密度函数&lt;br&gt;
满足&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$P{X \in D} = \int_{D} f(x) {dx}$&lt;br&gt;
&lt;/p&gt;
&lt;h3&gt;分布&lt;/h3&gt;
&lt;h4&gt;均匀分布&lt;/h4&gt;
&lt;p&gt;f(x) = 1/(b-a),  x在（a,b)内， 其他的地方为0&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;$X \sim U(a,b)$&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;正态分布 （normal）&lt;/h4&gt;
&lt;p&gt;$X \sim N(\mu, \sigma^2)$&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;性质&lt;br&gt;
- f(x) 关于 $x=\mu$ 对称&lt;br&gt;
- $\sigma$越大, 最大值越大, 就是越瘦高.  &lt;br&gt;
- N(0, 1) 是标准正态分布&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;指数分布&lt;/h4&gt;
&lt;p&gt;$f(x) = \lambda e^{-\lambda x}$, x&amp;gt;0&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;记作$X \sim E(\lambda)$&lt;/p&gt;
&lt;h4&gt;others&lt;/h4&gt;
&lt;p&gt;比如 Gamma分布， 
$X \sim \Gamma(\alpha, \beta)$&lt;/p&gt;
&lt;p&gt;Beta 分布&lt;br&gt;
$X \sim \beta(a, b)$&lt;/p&gt;</content></entry><entry><title>Ng Machine Learning Week 11</title><link href="http://littlezz.github.io/ng-machine-learning-week-11.html" rel="alternate"></link><published>2016-01-24T20:04:20+08:00</published><updated>2016-01-24T20:04:20+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-01-24:/ng-machine-learning-week-11.html</id><summary type="html">&lt;p&gt;第十一周说的是处理图片， 介绍了滑动窗口， 如何获得（制造）更多的数据， 还有在pipeline 中的上限分析， 这个是用来决定在系统中应该跟加关心那些模块的优化， 哪些模块就算优化了效果也比较小。  当然了， 喜闻乐见的黑了一下『我认识的人』， 花了18个月去优化一个没有太大效果的模块。&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;滑动窗口&lt;/h4&gt;
&lt;p&gt;图片处理的时候是训练识别一小块， 然后滑动窗口来在大图中寻找是否找到。 比如你的样本都是 10x10 的大小， 然后等比例缩放， 在原图中你也可以找 50x50 的, 但是对于这个范围， 你得缩小到10x10， 然后在来识别。&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;展开&lt;/h4&gt;
&lt;p&gt;里面还顺带提及了， 什么展开（好吧我不记得具体的名字了）， 在识别图像里面的文字的时候， 首先要找到包含文字的区域， 比如标记为白色， 然后这些白色向周围扩散， 比如20个像素点， 如果这之中也有其他的文字区域（即白色）， 着把他们连成一块大的白色区域。&lt;/p&gt;
&lt;h4&gt;制造数据&lt;/h4&gt;
&lt;p&gt;用现有样本， 加上噪声之类的， 得到新的训练数据。&lt;br&gt;
他建议在解决问题前， 先算一下如果我要得到10倍的训练数据， 我需要多大的代价， 比如人工10秒做一次判断， 那么如果我有新的10k条数据， 可能就是人工工作几天， 但是我得到了更加多的数据， 也就是说， 我的训练结果得到了一个巨大的提升。 &lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;上限分析&lt;/h4&gt;
&lt;p&gt;先要模块化， 然后重头开始给于一个模块绝对正确的预测结果给下一个模块， 计算其中系统正确率的变化。&lt;br&gt;
然后就可以明确的知道哪些模块不需要花精力啦（~~前提是你得弄成pipeline， 一般自己写的话都是大杂烩吧~~）&lt;br&gt;
同时不忘黑那个18个月的哥们，~~这家伙我好想在第一周的时候感觉就听说过他了~~。&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;终结&lt;/h4&gt;
&lt;p&gt;最后的最后， Andrew 结束词的时候， 我非常的感动（才没有哭！）， 这是我大学之后， 上过的第一门真正体现『传道， 授业， 解惑』的课。~~（大学上的那个不叫课， 叫吃屎）~~ &lt;/p&gt;
&lt;p&gt;我还记得第三周的时候， 介绍各种优化， 他坦然的承认自己搞了10年对于这些优化也是最近才渐渐明白里面的原理， 我隔着屏幕， 跨越时空， 哈哈哈的笑着。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;我还记得， 当我们上完线性回归和logic回归的时候， 他说， 你们现在已经比大多数硅谷工作的工程师更加懂机器学习了， 他们中有很多人靠机器学习赚到了许多钱， 或者帮助公司取得成功。 然后露出一个诙谐的笑容时候， 我也跟着笑着， 这家伙， 整天黑硅谷的人 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;第十一周说的是处理图片， 介绍了滑动窗口， 如何获得（制造）更多的数据， 还有在pipeline 中的上限分析， 这个是用来决定在系统中应该跟加关心那些模块的优化， 哪些模块就算优化了效果也比较小。  当然了， 喜闻乐见的黑了一下『我认识的人』， 花了18个月去优化一个没有太大效果的模块。&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;滑动窗口&lt;/h4&gt;
&lt;p&gt;图片处理的时候是训练识别一小块， 然后滑动窗口来在大图中寻找是否找到。 比如你的样本都是 10x10 的大小， 然后等比例缩放， 在原图中你也可以找 50x50 的, 但是对于这个范围， 你得缩小到10x10， 然后在来识别。&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;展开&lt;/h4&gt;
&lt;p&gt;里面还顺带提及了， 什么展开（好吧我不记得具体的名字了）， 在识别图像里面的文字的时候， 首先要找到包含文字的区域， 比如标记为白色， 然后这些白色向周围扩散， 比如20个像素点， 如果这之中也有其他的文字区域（即白色）， 着把他们连成一块大的白色区域。&lt;/p&gt;
&lt;h4&gt;制造数据&lt;/h4&gt;
&lt;p&gt;用现有样本， 加上噪声之类的， 得到新的训练数据。&lt;br&gt;
他建议在解决问题前， 先算一下如果我要得到10倍的训练数据， 我需要多大的代价， 比如人工10秒做一次判断， 那么如果我有新的10k条数据， 可能就是人工工作几天， 但是我得到了更加多的数据， 也就是说， 我的训练结果得到了一个巨大的提升。 &lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;上限分析&lt;/h4&gt;
&lt;p&gt;先要模块化， 然后重头开始给于一个模块绝对正确的预测结果给下一个模块， 计算其中系统正确率的变化。&lt;br&gt;
然后就可以明确的知道哪些模块不需要花精力啦（~~前提是你得弄成pipeline， 一般自己写的话都是大杂烩吧~~）&lt;br&gt;
同时不忘黑那个18个月的哥们，~~这家伙我好想在第一周的时候感觉就听说过他了~~。&lt;br&gt;
&lt;/p&gt;
&lt;h4&gt;终结&lt;/h4&gt;
&lt;p&gt;最后的最后， Andrew 结束词的时候， 我非常的感动（才没有哭！）， 这是我大学之后， 上过的第一门真正体现『传道， 授业， 解惑』的课。~~（大学上的那个不叫课， 叫吃屎）~~ &lt;/p&gt;
&lt;p&gt;我还记得第三周的时候， 介绍各种优化， 他坦然的承认自己搞了10年对于这些优化也是最近才渐渐明白里面的原理， 我隔着屏幕， 跨越时空， 哈哈哈的笑着。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;我还记得， 当我们上完线性回归和logic回归的时候， 他说， 你们现在已经比大多数硅谷工作的工程师更加懂机器学习了， 他们中有很多人靠机器学习赚到了许多钱， 或者帮助公司取得成功。 然后露出一个诙谐的笑容时候， 我也跟着笑着， 这家伙， 整天黑硅谷的人。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;我还记得当他鼓励完我们， 接着告诉我们， 他还有更厉害的东西要教授我们的时候， 我真的觉得接下来要得到巨大力量， 倍感兴奋。 &lt;/p&gt;
&lt;p&gt;就在最后， 他说， 同学们， 谢谢你们， 我知道你们当中很多人花了大量的时间的来学习， 同时花了大量的时间做哪些长度很长的编程题目，希望你们能在自己的领域利用机器学习取得成功， 希望你们能够让更多的人生活得更美好的时候。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;我觉得这个祝福一点也不畸形， 我觉得他有资格对他的学生们说希望你们改变世界， 当他说出这句话的时候， 我没有丝毫的觉得矫揉造作， 而是觉得， 理应如此。&lt;/p&gt;
&lt;p&gt;因为，&lt;br&gt;
师者，传道授业解惑者也。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;我知道我已经严重偏题了， 这些东西都应该归类到个人日记那一块， 而且我也知道反正也没有人看，但是我还是想在一个属于『公开』的地方， 写下此时此刻的想法。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;我觉得人的能力真的不是最重要的， 最重要的是他对这个世界， 对其他的人，对周边的事物还存有的一份爱意， 还相信自己在任何情况下都存在着可能性。
这些才是最重要的， 那些所谓的能力， 终究是科学训练的产物， 这个世界对于我们真正的考验， 是科学之外的东西。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;所以， 爱因斯坦说， &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一个人在智力上的成就很大程度上取决于人格的伟大，这一点往往超出人们通常的认识。&lt;br&gt;
&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因为，我们还有很长的路要走。&lt;/p&gt;
&lt;p&gt;以上。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;2016年01月24日21:29:36&lt;/p&gt;</content><category term="机器学习"></category><category term="瞎扯"></category><category term="日记"></category></entry><entry><title>Ng Machine Learning Week 10</title><link href="http://littlezz.github.io/ng-machine-learning-week-10.html" rel="alternate"></link><published>2016-01-24T14:49:33+08:00</published><updated>2016-01-24T14:49:33+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-01-24:/ng-machine-learning-week-10.html</id><summary type="html">&lt;p&gt;总结一下第10周， 之前看的都没有总结（←懒 &lt;br&gt;
&lt;/p&gt;
&lt;p&gt;第十周已经开始不怎么说算法了， 这次说的是对于大数据集的处理， 之前那种每次都对所有数据集做计算的称为batch， 现在这个（stochastic）是每次只对一个样例找最优值， 遍历完所有样例之后也会趋向于全局最优， 会在他的旁边震荡， 这个时候可以动态的减少学习效率 (alpha) &lt;br&gt;
&lt;/p&gt;
&lt;p&gt;之后说了mini-batch， 就是stochastic 和batch的混合版， 每次只训练一小部分样例。&lt;/p&gt;
&lt;p&gt;然后在线的学习其实感觉就是stochastic， 每次都对一个新的样本训练， 然后丢弃。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;之后是map reduce， 这个没有什么好说的了， 分布式计算， 或者并行的多核计算， 分布式要考虑网络带宽等问题， 虽然视频说多核的不用考虑带宽， 但是实际上核心之间的通信也要消耗时间， 最终可能每个核心只能快80%吧。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;map reduce的使用需要计算的时候数据能够分割， 一般是有 Σ 这种求和公式的时候使用。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;嘛。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;2016年01月24日14:42:29&lt;/p&gt;</summary><content type="html">&lt;p&gt;总结一下第10周， 之前看的都没有总结（←懒 &lt;br&gt;
&lt;/p&gt;
&lt;p&gt;第十周已经开始不怎么说算法了， 这次说的是对于大数据集的处理， 之前那种每次都对所有数据集做计算的称为batch， 现在这个（stochastic）是每次只对一个样例找最优值， 遍历完所有样例之后也会趋向于全局最优， 会在他的旁边震荡， 这个时候可以动态的减少学习效率 (alpha) &lt;br&gt;
&lt;/p&gt;
&lt;p&gt;之后说了mini-batch， 就是stochastic 和batch的混合版， 每次只训练一小部分样例。&lt;/p&gt;
&lt;p&gt;然后在线的学习其实感觉就是stochastic， 每次都对一个新的样本训练， 然后丢弃。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;之后是map reduce， 这个没有什么好说的了， 分布式计算， 或者并行的多核计算， 分布式要考虑网络带宽等问题， 虽然视频说多核的不用考虑带宽， 但是实际上核心之间的通信也要消耗时间， 最终可能每个核心只能快80%吧。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;map reduce的使用需要计算的时候数据能够分割， 一般是有 Σ 这种求和公式的时候使用。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;嘛。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;2016年01月24日14:42:29&lt;/p&gt;</content><category term="机器学习"></category></entry><entry><title>ML-ex8</title><link href="http://littlezz.github.io/ml-ex8.html" rel="alternate"></link><published>2016-01-24T00:00:00+08:00</published><updated>2016-01-24T00:00:00+08:00</updated><author><name>littlezz</name></author><id>tag:littlezz.github.io,2016-01-24:/ml-ex8.html</id><summary type="html">&lt;p&gt;今天撸了Ng的机器学习的第九周的作业。 课之前看了， 作业拖到我学校的考试周结束了才做。 诶一提起这个就想到我挂科估计要挂到退学警告了。。。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;这一周的是关于异常检查和推荐系统的， 协同过滤我之前看过诶， 和这个不一样， Ng还是求那个gradient 什么鬼的， 可是我在另一篇&lt;a href="http://www.salemmarafi.com/code/collaborative-filtering-with-python/"&gt;文章&lt;/a&gt;里面看到的是另一个东西。&lt;/p&gt;
&lt;p&gt;他也是用movie作为行， 用户作为列， 而且是用户直接当做了feature， 然后算两个movie之间的距离， 不过这样说来， 好像和机器学习就没有关系了。。。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;另外， 每次做作业的感觉其实就是， 我不会机器学习， 我只是公式的搬运工....  T_T&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;说一下搬运工的心得吧。&lt;br&gt;
今天把公式转换成矩阵的向量运算的时候折腾了好久。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;后来我还是没有发现什么取巧的办法， google了一下也没有找到好的技巧（←其实就找了一下）， 总结起来， 还是pdf里面说的&lt;br&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先用for 循环&lt;/li&gt;
&lt;li&gt;2维的可以直接matrix&lt;/li&gt;
&lt;li&gt;3维带一个for loop&lt;/li&gt;
&lt;li&gt;~~一般矩阵相乘形式上正确结果就是正确的~~&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实我花了半个小时在想如何消除3层嵌套的for循环。  /(ㄒoㄒ)/~~&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;嘛， 接着看吧。（~~弱逼不敢乱说话~~）&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;2016年01月24日00:57:49&lt;/p&gt;</summary><content type="html">&lt;p&gt;今天撸了Ng的机器学习的第九周的作业。 课之前看了， 作业拖到我学校的考试周结束了才做。 诶一提起这个就想到我挂科估计要挂到退学警告了。。。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;这一周的是关于异常检查和推荐系统的， 协同过滤我之前看过诶， 和这个不一样， Ng还是求那个gradient 什么鬼的， 可是我在另一篇&lt;a href="http://www.salemmarafi.com/code/collaborative-filtering-with-python/"&gt;文章&lt;/a&gt;里面看到的是另一个东西。&lt;/p&gt;
&lt;p&gt;他也是用movie作为行， 用户作为列， 而且是用户直接当做了feature， 然后算两个movie之间的距离， 不过这样说来， 好像和机器学习就没有关系了。。。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;另外， 每次做作业的感觉其实就是， 我不会机器学习， 我只是公式的搬运工....  T_T&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;说一下搬运工的心得吧。&lt;br&gt;
今天把公式转换成矩阵的向量运算的时候折腾了好久。&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;后来我还是没有发现什么取巧的办法， google了一下也没有找到好的技巧（←其实就找了一下）， 总结起来， 还是pdf里面说的&lt;br&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先用for 循环&lt;/li&gt;
&lt;li&gt;2维的可以直接matrix&lt;/li&gt;
&lt;li&gt;3维带一个for loop&lt;/li&gt;
&lt;li&gt;~~一般矩阵相乘形式上正确结果就是正确的~~&lt;br&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实我花了半个小时在想如何消除3层嵌套的for循环。  /(ㄒoㄒ)/~~&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;嘛， 接着看吧。（~~弱逼不敢乱说话~~）&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;2016年01月24日00:57:49&lt;/p&gt;</content><category term="机器学习"></category></entry></feed>