<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>zz's Blog</title><link href="http://littlezz.github.io/" rel="alternate"></link><link href="http://littlezz.github.io/feeds/machine-learning.atom.xml" rel="self"></link><id>http://littlezz.github.io/</id><updated>2016-01-24T20:04:20+08:00</updated><entry><title>Ng Machine Learning Week 11</title><link href="http://littlezz.github.io/ng-machine-learning-week-11.html" rel="alternate"></link><updated>2016-01-24T20:04:20+08:00</updated><author><name>zz</name></author><id>tag:littlezz.github.io,2016-01-24:ng-machine-learning-week-11.html</id><summary type="html">&lt;p&gt;第十一周说的是处理图片， 介绍了滑动窗口， 如何获得（制造）更多的数据， 还有在pipeline 中的上限分析， 这个是用来决定在系统中应该跟加关心那些模块的优化， 哪些模块就算优化了效果也比较小。  当然了， 喜闻乐见的黑了一下『我认识的人』， 花了18个月去优化一个没有太大效果的模块。  &lt;/p&gt;
&lt;h4 id="_1"&gt;&lt;a name="user-content-_1" href="#_1" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;滑动窗口&lt;/h4&gt;
&lt;p&gt;图片处理的时候是训练识别一小块， 然后滑动窗口来在大图中寻找是否找到。 比如你的样本都是 10x10 的大小， 然后等比例缩放， 在原图中你也可以找 50x50 的, 但是对于这个范围， 你得缩小到10x10， 然后在来识别。  &lt;/p&gt;
&lt;h4 id="_2"&gt;&lt;a name="user-content-_2" href="#_2" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;展开&lt;/h4&gt;
&lt;p&gt;里面还顺带提及了， 什么展开（好吧我不记得具体的名字了）， 在识别图像里面的文字的时候， 首先要找到包含文字的区域， 比如标记为白色， 然后这些白色向周围扩散， 比如20个像素点， 如果这之中也有其他的文字区域（即白色）， 着把他们连成一块大的白色区域。&lt;/p&gt;
&lt;h4 id="_3"&gt;&lt;a name="user-content-_3" href="#_3" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;制造数据&lt;/h4&gt;
&lt;p&gt;用现有样本， 加上噪声之类的， 得到新的训练数据。&lt;br /&gt;
他建议在解决问题前， 先算一下如果我要得到10倍的训练数据， 我需要多大的代价， 比如人工10秒做一次判断， 那么如果我有新的10k条数据， 可能就是人工工作几天， 但是我得到了更加多的数据， 也就是说， 我的训练结果得到了一个巨大的提升。   &lt;/p&gt;
&lt;h4 id="_4"&gt;&lt;a name="user-content-_4" href="#_4" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;上限分析&lt;/h4&gt;
&lt;p&gt;先要模块化， 然后重头开始给于一个模块绝对正确的预测结果给下一个模块， 计算其中系统正确率的变化。  &lt;/p&gt;
&lt;p&gt;然后就可以明确的知道哪些模块不需要花精力啦（&lt;del&gt;前提是你得弄成pipeline， 一般自己写的话都是大杂烩吧&lt;/del&gt;）&lt;/p&gt;
&lt;p&gt;同时不忘黑那个18个月的哥们，&lt;del&gt;这家伙我好想在第一周的时候感觉就听说过他了&lt;/del&gt;。&lt;/p&gt;
&lt;h4 id="_5"&gt;&lt;a name="user-content-_5" href="#_5" class="headeranchor-link" aria-hidden="true"&gt;&lt;span class="headeranchor"&gt;&lt;/span&gt;&lt;/a&gt;终结&lt;/h4&gt;
&lt;p&gt;最后的最后， Andrew 结束词的时候， 我非常的感动（才没有哭！）， 这是我大学之后， 上过的第一门真正体现『传道， 授业， 解惑』的课。&lt;del&gt;（大学上的那个不叫课， 叫吃屎）&lt;/del&gt; &lt;/p&gt;
&lt;p&gt;我还记得第三周的时候， 介绍各种优化， 他坦然的承认自己搞了10年对于这些优化也是最近才渐渐明白里面的原理， 我隔着屏幕， 跨越时空， 哈哈哈的笑着。  &lt;/p&gt;
&lt;p&gt;我还记得， 当我们上完线性回归和logic回归的时候， 他说， 你们现在已经比大多数硅谷工作的工程师更加懂机器学习了， 他们中有很多人靠机器学习赚到了许多钱， 或者帮助公司取得成功。 然后露出一个诙谐的笑容时候， 我也跟着笑着， 这家伙， 整天黑硅谷的人。  &lt;/p&gt;
&lt;p&gt;我还记得当他鼓励完我们， 接着告诉我们， 他还有更厉害的东西要教授我们的时候， 我真的觉得接下来要得到巨大力量， 倍感兴奋。 &lt;/p&gt;
&lt;p&gt;就在最后， 他说， 同学们， 谢谢你们， 我知道你们当中很多人花了大量的时间的来学习， 同时花了大量的时间做哪些长度很长的编程题目，希望你们能在自己的领域利用机器学习取得成功， 希望你们能够让更多的人生活得更美好的时候。  &lt;/p&gt;
&lt;p&gt;我觉得这个祝福一点也不畸形， 我觉得他有资格对他的学生们说希望你们改变世界， 当他说出这句话的时候， 我没有丝毫的觉得矫揉造作， 而是觉得， 理应如此。&lt;/p&gt;
&lt;p&gt;因为，&lt;br /&gt;
师者，传道授业解惑者也。  &lt;/p&gt;
&lt;p&gt;我知道我已经严重偏题了， 这些东西都应该归类到个人日记那一块， 而且我也知道反正也没有人看，但是我还是想在一个属于『公开』的地方， 写下此时此刻的想法。  &lt;/p&gt;
&lt;p&gt;我觉得人的能力真的不是最重要的， 最重要的是他对这个世界， 对其他的人，对周边的事物还存有的一份爱意， 还相信自己在任何情况下都存在着可能性。&lt;br /&gt;
这些才是最重要的， 那些所谓的能力， 终究是科学训练的产物， 这个世界对于我们真正的考验， 是科学之外的东西。  &lt;/p&gt;
&lt;p&gt;所以， 爱因斯坦说， &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;一个人在智力上的成就很大程度上取决于人格的伟大，这一点往往超出人们通常的认识。  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;因为，我们还有很长的路要走。&lt;/p&gt;
&lt;p&gt;以上。  &lt;/p&gt;
&lt;p&gt;2016年01月24日21:29:36&lt;/p&gt;</summary><category term="机器学习"></category><category term="瞎扯"></category></entry><entry><title>Ng Machine Learning Week 10</title><link href="http://littlezz.github.io/ng-machine-learning-week-10.html" rel="alternate"></link><updated>2016-01-24T14:49:33+08:00</updated><author><name>zz</name></author><id>tag:littlezz.github.io,2016-01-24:ng-machine-learning-week-10.html</id><summary type="html">&lt;p&gt;总结一下第10周， 之前看的都没有总结（←懒   &lt;/p&gt;
&lt;p&gt;第十周已经开始不怎么说算法了， 这次说的是对于大数据集的处理， 之前那种每次都对所有数据集做计算的称为batch， 现在这个（stochastic）是每次只对一个样例找最优值， 遍历完所有样例之后也会趋向于全局最优， 会在他的旁边震荡， 这个时候可以动态的减少学习效率 (alpha)   &lt;/p&gt;
&lt;p&gt;之后说了mini-batch， 就是stochastic 和batch的混合版， 每次只训练一小部分样例。&lt;/p&gt;
&lt;p&gt;然后在线的学习其实感觉就是stochastic， 每次都对一个新的样本训练， 然后丢弃。  &lt;/p&gt;
&lt;p&gt;之后是map reduce， 这个没有什么好说的了， 分布式计算， 或者并行的多核计算， 分布式要考虑网络带宽等问题， 虽然视频说多核的不用考虑带宽， 但是实际上核心之间的通信也要消耗时间， 最终可能每个核心只能快80%吧。  &lt;/p&gt;
&lt;p&gt;map reduce的使用需要计算的时候数据能够分割， 一般是有 Σ 这种求和公式的时候使用。  &lt;/p&gt;
&lt;p&gt;嘛。  &lt;/p&gt;
&lt;p&gt;2016年01月24日14:42:29&lt;/p&gt;</summary><category term="机器学习"></category></entry><entry><title>ML-ex8</title><link href="http://littlezz.github.io/ml-ex8.html" rel="alternate"></link><updated>2016-01-24T00:00:00+08:00</updated><author><name>zz</name></author><id>tag:littlezz.github.io,2016-01-24:ml-ex8.html</id><summary type="html">&lt;p&gt;今天撸了Ng的机器学习的第九周的作业。 课之前看了， 作业拖到我学校的考试周结束了才做。 诶一提起这个就想到我挂科估计要挂到退学警告了。。。  &lt;/p&gt;
&lt;p&gt;这一周的是关于异常检查和推荐系统的， 协同过滤我之前看过诶， 和这个不一样， Ng还是求那个gradient 什么鬼的， 可是我在另一篇&lt;a href="http://www.salemmarafi.com/code/collaborative-filtering-with-python/"&gt;文章&lt;/a&gt;里面看到的是另一个东西。&lt;/p&gt;
&lt;p&gt;他也是用movie作为行， 用户作为列， 而且是用户直接当做了feature， 然后算两个movie之间的距离， 不过这样说来， 好像和机器学习就没有关系了。。。  &lt;/p&gt;
&lt;p&gt;另外， 每次做作业的感觉其实就是， 我不会机器学习， 我只是公式的搬运工....  T_T  &lt;/p&gt;
&lt;p&gt;说一下搬运工的心得吧。&lt;br /&gt;
今天把公式转换成矩阵的向量运算的时候折腾了好久。  &lt;/p&gt;
&lt;p&gt;后来我还是没有发现什么取巧的办法， google了一下也没有找到好的技巧（←其实就找了一下）， 总结起来， 还是pdf里面说的  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先用for 循环&lt;/li&gt;
&lt;li&gt;2维的可以直接matrix&lt;/li&gt;
&lt;li&gt;3维带一个for loop&lt;/li&gt;
&lt;li&gt;&lt;del&gt;一般矩阵相乘形式上正确结果就是正确的&lt;/del&gt;  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其实我花了半个小时在想如何消除3层嵌套的for循环。  /(ㄒoㄒ)/~~  &lt;/p&gt;
&lt;p&gt;嘛， 接着看吧。（&lt;del&gt;弱逼不敢乱说话&lt;/del&gt;）  &lt;/p&gt;
&lt;p&gt;2016年01月24日00:57:49&lt;/p&gt;</summary><category term="机器学习"></category></entry></feed>